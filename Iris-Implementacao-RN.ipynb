{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes Neurais - Caso Iris**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contém a implementação de uma rede neural escrita explicitamente, que foi apresentado no trabalho da disciplina de Estatística Multivariável da EEL-USP. Aqui serão apresentados todos os processos que envolvem o aprendizado de uma rede artificial, bem como os conceitos mais importantes.\n",
    "\n",
    "A implementação foi feita em linguagem `R`, de forma que ao longo do notebook serão apresentadas algumas funções e detalhes do código para aqueles que não estão familiarizados. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importação e tratamento dos dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook utilizaremos o dataset [iris](https://archive.ics.uci.edu/ml/datasets/iris) como exemplo. Ele relaciona o comprimento e a largura de sépalas e pétalas de diversas flores de íris, e classifica-as entre três classes: **setosa**, **versicolor** e **virginica**. \n",
    "\n",
    "Iniciaremos importando os pacotes `pracma` e `datasets`, usados para realizar operações vetoriais e para importação de bases de código aberto, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"pracma\")\n",
    "library(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importar o dataset, basta entrar o comando `datasets::iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa\\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa\\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa\\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa\\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa\\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; | Species &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 5.1 | 3.5 | 1.4 | 0.2 | setosa |\n",
       "| 2 | 4.9 | 3.0 | 1.4 | 0.2 | setosa |\n",
       "| 3 | 4.7 | 3.2 | 1.3 | 0.2 | setosa |\n",
       "| 4 | 4.6 | 3.1 | 1.5 | 0.2 | setosa |\n",
       "| 5 | 5.0 | 3.6 | 1.4 | 0.2 | setosa |\n",
       "| 6 | 5.4 | 3.9 | 1.7 | 0.4 | setosa |\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1 5.1          3.5         1.4          0.2         setosa \n",
       "2 4.9          3.0         1.4          0.2         setosa \n",
       "3 4.7          3.2         1.3          0.2         setosa \n",
       "4 4.6          3.1         1.5          0.2         setosa \n",
       "5 5.0          3.6         1.4          0.2         setosa \n",
       "6 5.4          3.9         1.7          0.4         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- datasets::iris\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar esta base, se faz necessário normalizar as colunas numéricas e transformar a coluna `Species` em `factor`. A normalização escalar pode ser feita com a função `scale()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data$Species = factor(data$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data$Sepal.Length = scale(data$Sepal.Length)\n",
    "data$Sepal.Width = scale(data$Sepal.Width)\n",
    "data$Petal.Length = scale(data$Petal.Length)\n",
    "data$Petal.Width = scale(data$Petal.Width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos também a função `train_test_split` para separar a base em sets de treinamento e de teste, certificando que as classes sejam distribuidas de forma proporcional em ambos os sets. A sintaxe para a definição de uma função no R é:\n",
    "\n",
    "``` R\n",
    "nome_da_funcao = function(\n",
    "    varivel_1,\n",
    "    varivel_2\n",
    "    # ...\n",
    "){\n",
    "    # argumento da função\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = function(data, train_fraction = 0.8, train = TRUE) {\n",
    "    unique_targets = unlist(unique(data[5]))\n",
    "    len_unique = length(unique_targets) \n",
    "    \n",
    "    for (i in 1:len_unique) {\n",
    "        unique_data = subset(data, data[5] == as.character(unique_targets)[i])\n",
    "        \n",
    "        total_rows = nrow(unique_data)\n",
    "        train_rows = train_fraction * total_rows\n",
    "        sample = 1:train_rows\n",
    "        if (train == TRUE) {\n",
    "            dataset = unique_data[sample, ]\n",
    "        } else {\n",
    "            dataset = unique_data[-sample, ]\n",
    "        }\n",
    "        \n",
    "        if (i == 1) {\n",
    "            data_out = dataset\n",
    "        }\n",
    "        else {\n",
    "            data_out = rbind(data_out, dataset)\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return(data_out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train <- train_test_split(data, 0.8, train = TRUE)\n",
    "test <- train_test_split(data, 0.8, train = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rede artificial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma rede neural consiste de dois processos básicos, chamados _feedforward_ e _backpropagation_. O processo de _feedforward_ consiste em alimentar os neurônios subsequentes da rede com informações que, após serem avaliadas através de uma função, chegam finalmente ao último neurônio, gerando o output das operações. Já o processo de _backpropagation_ consiste em atualizar os pesos que estão associados a cada neurônio através de um algoritmo de otimização, com a finalidade de reajustar o modelo para gerar melhores resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo simples de rede neural é aquele que consegue classificar uma instância entre duas classes, ou seja, uma *classificação binária*. Como este dataset possui três classes como variável resposta, buscaremos responder à pergunta \"*Esta flor pertence à classe X?*\"\n",
    "Para tal, definiremos a função `distinguir(objeto, coluna)` que substitui os valores de classe para 0 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 3 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Species</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>setosa    </td></tr>\n",
       "\t<tr><th scope=row>51</th><td>versicolor</td></tr>\n",
       "\t<tr><th scope=row>101</th><td>virginica </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 3 × 1\n",
       "\\begin{tabular}{r|l}\n",
       "  & Species\\\\\n",
       "  & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & setosa    \\\\\n",
       "\t51 & versicolor\\\\\n",
       "\t101 & virginica \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 3 × 1\n",
       "\n",
       "| <!--/--> | Species &lt;fct&gt; |\n",
       "|---|---|\n",
       "| 1 | setosa     |\n",
       "| 51 | versicolor |\n",
       "| 101 | virginica  |\n",
       "\n"
      ],
      "text/plain": [
       "    Species   \n",
       "1   setosa    \n",
       "51  versicolor\n",
       "101 virginica "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique(train['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinguir = function(objeto, coluna) {\n",
    "    y = vector()\n",
    "    for (i in 1:nrow(coluna)) {\n",
    "        if (coluna[i, 1] == objeto) {\n",
    "            y[i] = 1\n",
    "        }\n",
    "        else {\n",
    "            y[i] = 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return (y)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input <- train[c(0:4)]\n",
    "output_setosa = distinguir('setosa', train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 1\n",
       "7. 1\n",
       "8. 1\n",
       "9. 1\n",
       "10. 1\n",
       "11. 1\n",
       "12. 1\n",
       "13. 1\n",
       "14. 1\n",
       "15. 1\n",
       "16. 1\n",
       "17. 1\n",
       "18. 1\n",
       "19. 1\n",
       "20. 1\n",
       "21. 1\n",
       "22. 1\n",
       "23. 1\n",
       "24. 1\n",
       "25. 1\n",
       "26. 1\n",
       "27. 1\n",
       "28. 1\n",
       "29. 1\n",
       "30. 1\n",
       "31. 1\n",
       "32. 1\n",
       "33. 1\n",
       "34. 1\n",
       "35. 1\n",
       "36. 1\n",
       "37. 1\n",
       "38. 1\n",
       "39. 1\n",
       "40. 1\n",
       "41. 0\n",
       "42. 0\n",
       "43. 0\n",
       "44. 0\n",
       "45. 0\n",
       "46. 0\n",
       "47. 0\n",
       "48. 0\n",
       "49. 0\n",
       "50. 0\n",
       "51. 0\n",
       "52. 0\n",
       "53. 0\n",
       "54. 0\n",
       "55. 0\n",
       "56. 0\n",
       "57. 0\n",
       "58. 0\n",
       "59. 0\n",
       "60. 0\n",
       "61. 0\n",
       "62. 0\n",
       "63. 0\n",
       "64. 0\n",
       "65. 0\n",
       "66. 0\n",
       "67. 0\n",
       "68. 0\n",
       "69. 0\n",
       "70. 0\n",
       "71. 0\n",
       "72. 0\n",
       "73. 0\n",
       "74. 0\n",
       "75. 0\n",
       "76. 0\n",
       "77. 0\n",
       "78. 0\n",
       "79. 0\n",
       "80. 0\n",
       "81. 0\n",
       "82. 0\n",
       "83. 0\n",
       "84. 0\n",
       "85. 0\n",
       "86. 0\n",
       "87. 0\n",
       "88. 0\n",
       "89. 0\n",
       "90. 0\n",
       "91. 0\n",
       "92. 0\n",
       "93. 0\n",
       "94. 0\n",
       "95. 0\n",
       "96. 0\n",
       "97. 0\n",
       "98. 0\n",
       "99. 0\n",
       "100. 0\n",
       "101. 0\n",
       "102. 0\n",
       "103. 0\n",
       "104. 0\n",
       "105. 0\n",
       "106. 0\n",
       "107. 0\n",
       "108. 0\n",
       "109. 0\n",
       "110. 0\n",
       "111. 0\n",
       "112. 0\n",
       "113. 0\n",
       "114. 0\n",
       "115. 0\n",
       "116. 0\n",
       "117. 0\n",
       "118. 0\n",
       "119. 0\n",
       "120. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       " [38] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       "[112] 0 0 0 0 0 0 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso podemos programar uma rede neural. O primeiro passo a se fazer é iniciar o processo de feedforward, o qual se baseia em uma operação matricial entre dois vetores: um vetor de parâmetros e um de pesos. O vetor de parâmetros toma os valores de entrada, que no nosso caso são as variáveis `Sepal.Lenght`, `Sepal.Width`, `Petal.Lenght` e `Petal.Width`. Chamando a i-ésima variável de $x_i$, temos uma operação linear dada por\n",
    "$$\n",
    "k(X, W) = x_1w_1+x_2w_2+x_3x_3+x_4w_4 = WX\n",
    "$$\n",
    "onde $X$ é o vetor $(x_1, x_2, x_3, x_4)$ e $W$$ = (w_1, w_2, w_3, w_4)$. O resultado desta função passa então por uma função chamada *função de ativação* $f(x)$, de forma que o resultado final do neurônio de saída é \n",
    "$$\n",
    "\\hat{y} = f(k(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversas funções de ativação, como a sigmoide, tangente hiperbólico, a ReLu, etc. Estas funções são usadas para introduzir [novos niveis de dimensionalidade nas redes](https://matheusfacure.github.io/2017/07/12/activ-func/), de forma que elas não sejam puramente operações lineares, tornando-as mais capacitadas a entenderem padrões complexos. A função que usaremos neste exemplo é a função de ativação **sigmoide**, que é dada pela função\n",
    "$$\n",
    "\\sigma(x) = \\dfrac{1}{1+e^{-x}}\n",
    "$$\n",
    "<center>\n",
    "<img src=\"figuras/Logistic-curve.png\">\n",
    "</center>\n",
    "Para a finalidade de otimização da rede, definiremos uma função para a função  sigmoid e também para a sua derivada, que é dada por $d\\sigma(x)/dx = \\sigma(x)(1-\\sigma(x))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = function(x){\n",
    "    return (1.0/(1 + exp(-x)))\n",
    "}\n",
    "\n",
    "logistic_deriv = function(x){\n",
    "    return (logistic(x)*(1-logistic(x)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de aprendizado de uma rede é baseado nos valores que os pesos $W$ possuem. Inicialmente, os valores destes pesos podem ser escolhidos aleatóriamente, o que pode ser realizado com a função `runif` que gera números aleatórios baseado na distribuição uniforme. Por finalidade de reprodutibilidade, é recomendável configurar um valor de semente (`seed`). Nos exemplos aqui apresentados tal valor será se $40$.\n",
    "\n",
    "Como o vetor de entrada possui $4$ dimensões, $W$ deve possuir também quatro dimensões. Chamaremos aqui o vetor $W$ de `weight_ItoO`, pois é o peso que é operado sobre os inputs, levando ao diretamente aos outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.683582016965374</li><li>0.87290378077887</li><li>0.690117288148031</li><li>0.11593607836403</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.683582016965374\n",
       "\\item 0.87290378077887\n",
       "\\item 0.690117288148031\n",
       "\\item 0.11593607836403\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.683582016965374\n",
       "2. 0.87290378077887\n",
       "3. 0.690117288148031\n",
       "4. 0.11593607836403\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.6835820 0.8729038 0.6901173 0.1159361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(40)\n",
    "\n",
    "weight_ItoO <- runif(4)\n",
    "weight_ItoO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso é possível realizar o produto interno entre os valores dos atributos com os pesos gerados aleatóriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Produto interno entre os atributos e pesos da primeira instância: \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "-0.800934443513676"
      ],
      "text/latex": [
       "-0.800934443513676"
      ],
      "text/markdown": [
       "-0.800934443513676"
      ],
      "text/plain": [
       "[1] -0.8009344"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Produto interno com a função de ativação: \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.309825667836144"
      ],
      "text/latex": [
       "0.309825667836144"
      ],
      "text/markdown": [
       "0.309825667836144"
      ],
      "text/plain": [
       "[1] 0.3098257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Produto interno entre os atributos e pesos da primeira instância: ')\n",
    "dot(unlist(input[1, ]), weight_ItoO)\n",
    "print('Produto interno com a função de ativação: ')\n",
    "logistic(dot(unlist(input[1, ]), weight_ItoO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, com os pesos aleatoriamente gerados o modelo inicial previu que a primeira flor contida no set `input` seria uma flor da espécie setosa. Obviamente este resultado não é confiável, pois ele foi gerado aleatóriamente e ainda não foi comparado com nenhum resultado real para validar a sua predição. Para que o processo de aprendizado ocorra, se faz necessário iniciar o processo de backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste processo deve ser implementado algum algoritmo de otimização. A técnica mais básica e comum em aplicações de aprendizado de máquina é o *gradiente descendente*. Este método consiste em tomar uma função que aponte para alguma métrica de erro, denominada *função custo*, e caminhar no espaço matemático dos pesos rumo ao ponto onde o gradiente desta função assume o menor valor, ou seja, onde a função custo é minima. \n",
    "\n",
    "Matematicamente falando, se a predição gerou um valor $\\hat{y}_i$, então a [função custo será](https://medium.com/@zeeshanmulla/cost-activation-loss-function-neural-network-deep-learning-what-are-these-91167825a4de) $J = \\sum^m_i(1/2)(\\hat{y}_i - y_i)^2$. Este valor implica que o processo de backpropagation não se baseia em apenas valores das instâncias, mas sim no que é gerado sobre todo o dataset. Portanto, se faz necessário criar um vetor contento os resultados do feedforward, calculados para cada instância. Chamaremos este vetor de predições de `outputP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputP_pre = vector()\n",
    "outputP = vector()\n",
    "for (i in 1:nrow(input)) {\n",
    "    outputP_pre[i] = dot(unlist(input[i, ]), weight_ItoO) ## pré-ativação\n",
    "    outputP[i] = logistic(outputP_pre[i]) ## pós-ativação\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a função sigmoid,[o gradiente da função custo é](https://towardsdatascience.com/a-step-by-step-implementation-of-gradient-descent-and-backpropagation-d58bda486110) ([este video](https://www.youtube.com/watch?time_continue=479&v=tIeHLnjs5U8&feature=emb_logo) apresenta uma derivação de tal resultado) \n",
    "$$\n",
    "\\dfrac{\\partial J(W)}{\\partial w_{j}} = (Y-\\hat{Y})\\dfrac{\\partial \\sigma(k(X, W))}{\\partial w_{j}} X_j\n",
    "$$\n",
    "onde $(Y-\\hat{Y}) = (y_1 - \\hat{y}_1, y_2 - \\hat{y}_2, \\dots)$, $X_j$ é o vetor coluna contendo todos os valores das instâncias para a j-ésima dimensão (um dos atributos de entrada do dataset) e $\\dfrac{\\partial \\sigma(k(X, W))}{\\partial w_{j}}$ é a derivada da função sigmoid, atuando sobre $k(x, w)$, em relação aos pesos. Esta derivada é\n",
    "$$\n",
    "\\dfrac{\\partial \\sigma(k(X, W))}{\\partial w_{j}} = \\sigma(k(X, W))(1 - \\sigma(k(X, W)))\n",
    "$$\n",
    "ficando claro agora o motivo da função `logistic_deriv` ter sido definida. Portanto, cada componente do vetor gradiente é resultado de um produto interno entre o vetor $X_j$ e $\\sigma(k(X, W))(1 - \\sigma(k(X, W))) (Y-\\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamando $(Y-\\hat{Y})$ de `erro`, $\\dfrac{\\partial \\sigma(k(X, W))}{\\partial w_{j}}(Y-\\hat{Y})$ de `S_e` e `grad` o vetor das gradiente $\\nabla J(W) = \\left(\\dfrac{\\partial J(W)}{\\partial w_{1}}, \\dfrac{\\partial J(W)}{\\partial w_{2}}, \\dots\\right)$, então:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-0.690174332163856</li><li>-0.877329553291865</li><li>-0.854587029664003</li><li>-0.878219237370641</li><li>-0.664456399308733</li><li>-0.40239501202954</li><li>-0.801985663333285</li><li>-0.739748395566388</li><li>-0.929589981505722</li><li>-0.851096739432281</li><li>-0.528381617727985</li><li>-0.763266275160394</li><li>-0.88746879674813</li><li>-0.930550824432147</li><li>-0.331793736377125</li><li>-0.172743478667022</li><li>-0.440502814818563</li><li>-0.68691255688697</li><li>-0.3946823854447</li><li>-0.536391741209575</li><li>-0.653909285968745</li><li>-0.581974348252672</li><li>-0.763108490293649</li><li>-0.738572331067704</li><li>-0.741425416871387</li><li>-0.858955607609072</li><li>-0.726144372984231</li><li>-0.663580057601713</li><li>-0.714767266116273</li><li>-0.839398578994265</li><li>-0.854641266152641</li><li>-0.664638634883142</li><li>-0.375870551482506</li><li>-0.282690853464567</li><li>-0.849158852937617</li><li>-0.826708819901338</li><li>-0.624764624492051</li><li>-0.685891060095758</li><li>-0.918282197032537</li><li>-0.723543988975736</li><li>0.837445942328455</li><li>0.74669439229639</li><li>0.809986550711338</li><li>0.15574806337035</li><li>0.599079126763469</li><li>0.418655090026698</li><li>0.784514831168426</li><li>0.0907479205444099</li><li>0.657918039544458</li><li>0.238544688610139</li><li>0.0499793891028995</li><li>0.537549964959388</li><li>0.178968491217803</li><li>0.573360528272783</li><li>0.362987367902867</li><li>0.745383803844778</li><li>0.50502705609543</li><li>0.343436338131771</li><li>0.252234344872204</li><li>0.218123455669825</li><li>0.696611065810121</li><li>0.451761021854801</li><li>0.438562236930074</li><li>0.516215997729448</li><li>0.591856695897696</li><li>0.688115039709505</li><li>0.670900687352785</li><li>0.760252492729615</li><li>0.537441449875339</li><li>0.23768757564684</li><li>0.168184440188262</li><li>0.16072373304102</li><li>0.3327474051249</li><li>0.49982009265567</li><li>0.463814787027157</li><li>0.762529934049684</li><li>0.769702565876024</li><li>0.294542305251798</li><li>0.458424934641801</li><li>0.215908026597154</li><li>0.874052895734041</li><li>0.469987876929554</li><li>0.869546202695395</li><li>0.705444404747</li><li>0.798630854247547</li><li>0.929783283768231</li><li>0.178187475934632</li><li>0.877882717022454</li><li>0.617908174613468</li><li>0.965113410872093</li><li>0.813698243084005</li><li>0.611424453897613</li><li>0.816516651757764</li><li>0.348153511838819</li><li>0.538952297546982</li><li>0.819872986333184</li><li>0.76846049153231</li><li>0.986905343757846</li><li>0.882107674103306</li><li>0.258004402402162</li><li>0.889399253897802</li><li>0.463066246633893</li><li>0.907983883572461</li><li>0.549628083821915</li><li>0.889865201986909</li><li>0.914784487733643</li><li>0.569004741826758</li><li>0.653593124571042</li><li>0.690246724013114</li><li>0.865804405186608</li><li>0.857152340102817</li><li>0.987128347392876</li><li>0.693489263196353</li><li>0.606349711756393</li><li>0.511650634372523</li><li>0.924202721564927</li><li>0.877176895543329</li><li>0.788741430331814</li><li>0.625562384915433</li><li>0.850261077913452</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -0.690174332163856\n",
       "\\item -0.877329553291865\n",
       "\\item -0.854587029664003\n",
       "\\item -0.878219237370641\n",
       "\\item -0.664456399308733\n",
       "\\item -0.40239501202954\n",
       "\\item -0.801985663333285\n",
       "\\item -0.739748395566388\n",
       "\\item -0.929589981505722\n",
       "\\item -0.851096739432281\n",
       "\\item -0.528381617727985\n",
       "\\item -0.763266275160394\n",
       "\\item -0.88746879674813\n",
       "\\item -0.930550824432147\n",
       "\\item -0.331793736377125\n",
       "\\item -0.172743478667022\n",
       "\\item -0.440502814818563\n",
       "\\item -0.68691255688697\n",
       "\\item -0.3946823854447\n",
       "\\item -0.536391741209575\n",
       "\\item -0.653909285968745\n",
       "\\item -0.581974348252672\n",
       "\\item -0.763108490293649\n",
       "\\item -0.738572331067704\n",
       "\\item -0.741425416871387\n",
       "\\item -0.858955607609072\n",
       "\\item -0.726144372984231\n",
       "\\item -0.663580057601713\n",
       "\\item -0.714767266116273\n",
       "\\item -0.839398578994265\n",
       "\\item -0.854641266152641\n",
       "\\item -0.664638634883142\n",
       "\\item -0.375870551482506\n",
       "\\item -0.282690853464567\n",
       "\\item -0.849158852937617\n",
       "\\item -0.826708819901338\n",
       "\\item -0.624764624492051\n",
       "\\item -0.685891060095758\n",
       "\\item -0.918282197032537\n",
       "\\item -0.723543988975736\n",
       "\\item 0.837445942328455\n",
       "\\item 0.74669439229639\n",
       "\\item 0.809986550711338\n",
       "\\item 0.15574806337035\n",
       "\\item 0.599079126763469\n",
       "\\item 0.418655090026698\n",
       "\\item 0.784514831168426\n",
       "\\item 0.0907479205444099\n",
       "\\item 0.657918039544458\n",
       "\\item 0.238544688610139\n",
       "\\item 0.0499793891028995\n",
       "\\item 0.537549964959388\n",
       "\\item 0.178968491217803\n",
       "\\item 0.573360528272783\n",
       "\\item 0.362987367902867\n",
       "\\item 0.745383803844778\n",
       "\\item 0.50502705609543\n",
       "\\item 0.343436338131771\n",
       "\\item 0.252234344872204\n",
       "\\item 0.218123455669825\n",
       "\\item 0.696611065810121\n",
       "\\item 0.451761021854801\n",
       "\\item 0.438562236930074\n",
       "\\item 0.516215997729448\n",
       "\\item 0.591856695897696\n",
       "\\item 0.688115039709505\n",
       "\\item 0.670900687352785\n",
       "\\item 0.760252492729615\n",
       "\\item 0.537441449875339\n",
       "\\item 0.23768757564684\n",
       "\\item 0.168184440188262\n",
       "\\item 0.16072373304102\n",
       "\\item 0.3327474051249\n",
       "\\item 0.49982009265567\n",
       "\\item 0.463814787027157\n",
       "\\item 0.762529934049684\n",
       "\\item 0.769702565876024\n",
       "\\item 0.294542305251798\n",
       "\\item 0.458424934641801\n",
       "\\item 0.215908026597154\n",
       "\\item 0.874052895734041\n",
       "\\item 0.469987876929554\n",
       "\\item 0.869546202695395\n",
       "\\item 0.705444404747\n",
       "\\item 0.798630854247547\n",
       "\\item 0.929783283768231\n",
       "\\item 0.178187475934632\n",
       "\\item 0.877882717022454\n",
       "\\item 0.617908174613468\n",
       "\\item 0.965113410872093\n",
       "\\item 0.813698243084005\n",
       "\\item 0.611424453897613\n",
       "\\item 0.816516651757764\n",
       "\\item 0.348153511838819\n",
       "\\item 0.538952297546982\n",
       "\\item 0.819872986333184\n",
       "\\item 0.76846049153231\n",
       "\\item 0.986905343757846\n",
       "\\item 0.882107674103306\n",
       "\\item 0.258004402402162\n",
       "\\item 0.889399253897802\n",
       "\\item 0.463066246633893\n",
       "\\item 0.907983883572461\n",
       "\\item 0.549628083821915\n",
       "\\item 0.889865201986909\n",
       "\\item 0.914784487733643\n",
       "\\item 0.569004741826758\n",
       "\\item 0.653593124571042\n",
       "\\item 0.690246724013114\n",
       "\\item 0.865804405186608\n",
       "\\item 0.857152340102817\n",
       "\\item 0.987128347392876\n",
       "\\item 0.693489263196353\n",
       "\\item 0.606349711756393\n",
       "\\item 0.511650634372523\n",
       "\\item 0.924202721564927\n",
       "\\item 0.877176895543329\n",
       "\\item 0.788741430331814\n",
       "\\item 0.625562384915433\n",
       "\\item 0.850261077913452\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -0.690174332163856\n",
       "2. -0.877329553291865\n",
       "3. -0.854587029664003\n",
       "4. -0.878219237370641\n",
       "5. -0.664456399308733\n",
       "6. -0.40239501202954\n",
       "7. -0.801985663333285\n",
       "8. -0.739748395566388\n",
       "9. -0.929589981505722\n",
       "10. -0.851096739432281\n",
       "11. -0.528381617727985\n",
       "12. -0.763266275160394\n",
       "13. -0.88746879674813\n",
       "14. -0.930550824432147\n",
       "15. -0.331793736377125\n",
       "16. -0.172743478667022\n",
       "17. -0.440502814818563\n",
       "18. -0.68691255688697\n",
       "19. -0.3946823854447\n",
       "20. -0.536391741209575\n",
       "21. -0.653909285968745\n",
       "22. -0.581974348252672\n",
       "23. -0.763108490293649\n",
       "24. -0.738572331067704\n",
       "25. -0.741425416871387\n",
       "26. -0.858955607609072\n",
       "27. -0.726144372984231\n",
       "28. -0.663580057601713\n",
       "29. -0.714767266116273\n",
       "30. -0.839398578994265\n",
       "31. -0.854641266152641\n",
       "32. -0.664638634883142\n",
       "33. -0.375870551482506\n",
       "34. -0.282690853464567\n",
       "35. -0.849158852937617\n",
       "36. -0.826708819901338\n",
       "37. -0.624764624492051\n",
       "38. -0.685891060095758\n",
       "39. -0.918282197032537\n",
       "40. -0.723543988975736\n",
       "41. 0.837445942328455\n",
       "42. 0.74669439229639\n",
       "43. 0.809986550711338\n",
       "44. 0.15574806337035\n",
       "45. 0.599079126763469\n",
       "46. 0.418655090026698\n",
       "47. 0.784514831168426\n",
       "48. 0.0907479205444099\n",
       "49. 0.657918039544458\n",
       "50. 0.238544688610139\n",
       "51. 0.0499793891028995\n",
       "52. 0.537549964959388\n",
       "53. 0.178968491217803\n",
       "54. 0.573360528272783\n",
       "55. 0.362987367902867\n",
       "56. 0.745383803844778\n",
       "57. 0.50502705609543\n",
       "58. 0.343436338131771\n",
       "59. 0.252234344872204\n",
       "60. 0.218123455669825\n",
       "61. 0.696611065810121\n",
       "62. 0.451761021854801\n",
       "63. 0.438562236930074\n",
       "64. 0.516215997729448\n",
       "65. 0.591856695897696\n",
       "66. 0.688115039709505\n",
       "67. 0.670900687352785\n",
       "68. 0.760252492729615\n",
       "69. 0.537441449875339\n",
       "70. 0.23768757564684\n",
       "71. 0.168184440188262\n",
       "72. 0.16072373304102\n",
       "73. 0.3327474051249\n",
       "74. 0.49982009265567\n",
       "75. 0.463814787027157\n",
       "76. 0.762529934049684\n",
       "77. 0.769702565876024\n",
       "78. 0.294542305251798\n",
       "79. 0.458424934641801\n",
       "80. 0.215908026597154\n",
       "81. 0.874052895734041\n",
       "82. 0.469987876929554\n",
       "83. 0.869546202695395\n",
       "84. 0.705444404747\n",
       "85. 0.798630854247547\n",
       "86. 0.929783283768231\n",
       "87. 0.178187475934632\n",
       "88. 0.877882717022454\n",
       "89. 0.617908174613468\n",
       "90. 0.965113410872093\n",
       "91. 0.813698243084005\n",
       "92. 0.611424453897613\n",
       "93. 0.816516651757764\n",
       "94. 0.348153511838819\n",
       "95. 0.538952297546982\n",
       "96. 0.819872986333184\n",
       "97. 0.76846049153231\n",
       "98. 0.986905343757846\n",
       "99. 0.882107674103306\n",
       "100. 0.258004402402162\n",
       "101. 0.889399253897802\n",
       "102. 0.463066246633893\n",
       "103. 0.907983883572461\n",
       "104. 0.549628083821915\n",
       "105. 0.889865201986909\n",
       "106. 0.914784487733643\n",
       "107. 0.569004741826758\n",
       "108. 0.653593124571042\n",
       "109. 0.690246724013114\n",
       "110. 0.865804405186608\n",
       "111. 0.857152340102817\n",
       "112. 0.987128347392876\n",
       "113. 0.693489263196353\n",
       "114. 0.606349711756393\n",
       "115. 0.511650634372523\n",
       "116. 0.924202721564927\n",
       "117. 0.877176895543329\n",
       "118. 0.788741430331814\n",
       "119. 0.625562384915433\n",
       "120. 0.850261077913452\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] -0.69017433 -0.87732955 -0.85458703 -0.87821924 -0.66445640 -0.40239501\n",
       "  [7] -0.80198566 -0.73974840 -0.92958998 -0.85109674 -0.52838162 -0.76326628\n",
       " [13] -0.88746880 -0.93055082 -0.33179374 -0.17274348 -0.44050281 -0.68691256\n",
       " [19] -0.39468239 -0.53639174 -0.65390929 -0.58197435 -0.76310849 -0.73857233\n",
       " [25] -0.74142542 -0.85895561 -0.72614437 -0.66358006 -0.71476727 -0.83939858\n",
       " [31] -0.85464127 -0.66463863 -0.37587055 -0.28269085 -0.84915885 -0.82670882\n",
       " [37] -0.62476462 -0.68589106 -0.91828220 -0.72354399  0.83744594  0.74669439\n",
       " [43]  0.80998655  0.15574806  0.59907913  0.41865509  0.78451483  0.09074792\n",
       " [49]  0.65791804  0.23854469  0.04997939  0.53754996  0.17896849  0.57336053\n",
       " [55]  0.36298737  0.74538380  0.50502706  0.34343634  0.25223434  0.21812346\n",
       " [61]  0.69661107  0.45176102  0.43856224  0.51621600  0.59185670  0.68811504\n",
       " [67]  0.67090069  0.76025249  0.53744145  0.23768758  0.16818444  0.16072373\n",
       " [73]  0.33274741  0.49982009  0.46381479  0.76252993  0.76970257  0.29454231\n",
       " [79]  0.45842493  0.21590803  0.87405290  0.46998788  0.86954620  0.70544440\n",
       " [85]  0.79863085  0.92978328  0.17818748  0.87788272  0.61790817  0.96511341\n",
       " [91]  0.81369824  0.61142445  0.81651665  0.34815351  0.53895230  0.81987299\n",
       " [97]  0.76846049  0.98690534  0.88210767  0.25800440  0.88939925  0.46306625\n",
       "[103]  0.90798388  0.54962808  0.88986520  0.91478449  0.56900474  0.65359312\n",
       "[109]  0.69024672  0.86580441  0.85715234  0.98712835  0.69348926  0.60634971\n",
       "[115]  0.51165063  0.92420272  0.87717690  0.78874143  0.62556238  0.85026108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "erro = outputP - output_setosa\n",
    "erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-0.147582547232065</li><li>-0.0944203193213214</li><li>-0.106197853813352</li><li>-0.0939257305303698</li><li>-0.148143273664133</li><li>-0.0967652428949415</li><li>-0.127359059901736</li><li>-0.142416883985875</li><li>-0.06084393973053</li><li>-0.107860408598267</li><li>-0.131669784466</li><li>-0.137915246051055</li><li>-0.0886296730433471</li><li>-0.0601377660230536</li><li>-0.0735608787380686</li><li>-0.02468559056793</li><li>-0.108566361163314</li><li>-0.147729953366484</li><li>-0.0942928583037096</li><li>-0.133387560164515</li><li>-0.14798747365931</li><li>-0.141582839462636</li><li>-0.137950114957826</li><li>-0.14260594076104</li><li>-0.142141460424984</li><li>-0.104063220670135</li><li>-0.144400142411273</li><li>-0.148138650399233</li><li>-0.145723191679007</li><li>-0.11315815111786</li><li>-0.10617171899967</li><li>-0.148144043576351</li><li>-0.0881761793129557</li><li>-0.0573231282324282</li><li>-0.10876714017781</li><li>-0.118435419114751</li><li>-0.146465937825056</li><li>-0.147771465945962</li><li>-0.0689078994130422</li><li>-0.14472911852288</li><li>0.114001713771491</li><li>0.141231178762079</li><li>0.12466368412858</li><li>0.0204794069441363</li><li>0.14388881758651</li><li>0.10189353412941</li><li>0.132623260586525</li><li>0.00748785916154172</li><li>0.148072279278216</li><li>0.0433295244440317</li><li>0.00237309385311468</li><li>0.133629545859968</li><li>0.0262974100343439</li><li>0.140254439235847</li><li>0.083932675639703</li><li>0.141463998563181</li><li>0.126244001337157</li><li>0.0774407111193878</li><li>0.0475744696923024</li><li>0.0371999986218945</li><li>0.147224630952339</li><li>0.111889008011961</li><li>0.107985162769522</li><li>0.128918256005407</li><li>0.142970292796558</li><li>0.147678248488869</li><li>0.14813014531398</li><li>0.13857018792774</li><li>0.133606943632327</li><li>0.0430671328497398</li><li>0.02352873985038</li><li>0.02168028386704</li><li>0.073878764872419</li><li>0.124955006986414</li><li>0.115346391757051</li><li>0.138077421116358</li><li>0.13643788165986</li><li>0.0612021019415432</li><li>0.113813852551577</li><li>0.0365514478016036</li><li>0.0962196158594576</li><li>0.11707363821231</li><li>0.0986374987724957</li><li>0.146586124589798</li><li>0.128435504823203</li><li>0.0607021373566533</li><li>0.0260931858422084</li><li>0.094113111329475</li><li>0.145886675578954</li><li>0.0324949004900476</li><li>0.123351293240234</li><li>0.145265028851675</li><li>0.12232824599906</li><li>0.0790108785061164</li><li>0.133920332044817</li><li>0.121079886003763</li><li>0.136731379506924</li><li>0.0127539615389493</li><li>0.0917336632263122</li><li>0.0493918805194031</li><li>0.0874886224214287</li><li>0.115134892001539</li><li>0.0758612823627466</li><li>0.136053316278094</li><li>0.087211349712913</li><li>0.0713109532867168</li><li>0.139541781527235</li><li>0.147979465142391</li><li>0.147579018081317</li><li>0.100595335167089</li><li>0.104951623365178</li><li>0.0125424262936534</li><li>0.147409398901295</li><li>0.144729454326247</li><li>0.127843208527055</li><li>0.0647422962010321</li><li>0.0945049242631224</li><li>0.131426711830857</li><li>0.146528034401455</li><li>0.108252840407067</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -0.147582547232065\n",
       "\\item -0.0944203193213214\n",
       "\\item -0.106197853813352\n",
       "\\item -0.0939257305303698\n",
       "\\item -0.148143273664133\n",
       "\\item -0.0967652428949415\n",
       "\\item -0.127359059901736\n",
       "\\item -0.142416883985875\n",
       "\\item -0.06084393973053\n",
       "\\item -0.107860408598267\n",
       "\\item -0.131669784466\n",
       "\\item -0.137915246051055\n",
       "\\item -0.0886296730433471\n",
       "\\item -0.0601377660230536\n",
       "\\item -0.0735608787380686\n",
       "\\item -0.02468559056793\n",
       "\\item -0.108566361163314\n",
       "\\item -0.147729953366484\n",
       "\\item -0.0942928583037096\n",
       "\\item -0.133387560164515\n",
       "\\item -0.14798747365931\n",
       "\\item -0.141582839462636\n",
       "\\item -0.137950114957826\n",
       "\\item -0.14260594076104\n",
       "\\item -0.142141460424984\n",
       "\\item -0.104063220670135\n",
       "\\item -0.144400142411273\n",
       "\\item -0.148138650399233\n",
       "\\item -0.145723191679007\n",
       "\\item -0.11315815111786\n",
       "\\item -0.10617171899967\n",
       "\\item -0.148144043576351\n",
       "\\item -0.0881761793129557\n",
       "\\item -0.0573231282324282\n",
       "\\item -0.10876714017781\n",
       "\\item -0.118435419114751\n",
       "\\item -0.146465937825056\n",
       "\\item -0.147771465945962\n",
       "\\item -0.0689078994130422\n",
       "\\item -0.14472911852288\n",
       "\\item 0.114001713771491\n",
       "\\item 0.141231178762079\n",
       "\\item 0.12466368412858\n",
       "\\item 0.0204794069441363\n",
       "\\item 0.14388881758651\n",
       "\\item 0.10189353412941\n",
       "\\item 0.132623260586525\n",
       "\\item 0.00748785916154172\n",
       "\\item 0.148072279278216\n",
       "\\item 0.0433295244440317\n",
       "\\item 0.00237309385311468\n",
       "\\item 0.133629545859968\n",
       "\\item 0.0262974100343439\n",
       "\\item 0.140254439235847\n",
       "\\item 0.083932675639703\n",
       "\\item 0.141463998563181\n",
       "\\item 0.126244001337157\n",
       "\\item 0.0774407111193878\n",
       "\\item 0.0475744696923024\n",
       "\\item 0.0371999986218945\n",
       "\\item 0.147224630952339\n",
       "\\item 0.111889008011961\n",
       "\\item 0.107985162769522\n",
       "\\item 0.128918256005407\n",
       "\\item 0.142970292796558\n",
       "\\item 0.147678248488869\n",
       "\\item 0.14813014531398\n",
       "\\item 0.13857018792774\n",
       "\\item 0.133606943632327\n",
       "\\item 0.0430671328497398\n",
       "\\item 0.02352873985038\n",
       "\\item 0.02168028386704\n",
       "\\item 0.073878764872419\n",
       "\\item 0.124955006986414\n",
       "\\item 0.115346391757051\n",
       "\\item 0.138077421116358\n",
       "\\item 0.13643788165986\n",
       "\\item 0.0612021019415432\n",
       "\\item 0.113813852551577\n",
       "\\item 0.0365514478016036\n",
       "\\item 0.0962196158594576\n",
       "\\item 0.11707363821231\n",
       "\\item 0.0986374987724957\n",
       "\\item 0.146586124589798\n",
       "\\item 0.128435504823203\n",
       "\\item 0.0607021373566533\n",
       "\\item 0.0260931858422084\n",
       "\\item 0.094113111329475\n",
       "\\item 0.145886675578954\n",
       "\\item 0.0324949004900476\n",
       "\\item 0.123351293240234\n",
       "\\item 0.145265028851675\n",
       "\\item 0.12232824599906\n",
       "\\item 0.0790108785061164\n",
       "\\item 0.133920332044817\n",
       "\\item 0.121079886003763\n",
       "\\item 0.136731379506924\n",
       "\\item 0.0127539615389493\n",
       "\\item 0.0917336632263122\n",
       "\\item 0.0493918805194031\n",
       "\\item 0.0874886224214287\n",
       "\\item 0.115134892001539\n",
       "\\item 0.0758612823627466\n",
       "\\item 0.136053316278094\n",
       "\\item 0.087211349712913\n",
       "\\item 0.0713109532867168\n",
       "\\item 0.139541781527235\n",
       "\\item 0.147979465142391\n",
       "\\item 0.147579018081317\n",
       "\\item 0.100595335167089\n",
       "\\item 0.104951623365178\n",
       "\\item 0.0125424262936534\n",
       "\\item 0.147409398901295\n",
       "\\item 0.144729454326247\n",
       "\\item 0.127843208527055\n",
       "\\item 0.0647422962010321\n",
       "\\item 0.0945049242631224\n",
       "\\item 0.131426711830857\n",
       "\\item 0.146528034401455\n",
       "\\item 0.108252840407067\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -0.147582547232065\n",
       "2. -0.0944203193213214\n",
       "3. -0.106197853813352\n",
       "4. -0.0939257305303698\n",
       "5. -0.148143273664133\n",
       "6. -0.0967652428949415\n",
       "7. -0.127359059901736\n",
       "8. -0.142416883985875\n",
       "9. -0.06084393973053\n",
       "10. -0.107860408598267\n",
       "11. -0.131669784466\n",
       "12. -0.137915246051055\n",
       "13. -0.0886296730433471\n",
       "14. -0.0601377660230536\n",
       "15. -0.0735608787380686\n",
       "16. -0.02468559056793\n",
       "17. -0.108566361163314\n",
       "18. -0.147729953366484\n",
       "19. -0.0942928583037096\n",
       "20. -0.133387560164515\n",
       "21. -0.14798747365931\n",
       "22. -0.141582839462636\n",
       "23. -0.137950114957826\n",
       "24. -0.14260594076104\n",
       "25. -0.142141460424984\n",
       "26. -0.104063220670135\n",
       "27. -0.144400142411273\n",
       "28. -0.148138650399233\n",
       "29. -0.145723191679007\n",
       "30. -0.11315815111786\n",
       "31. -0.10617171899967\n",
       "32. -0.148144043576351\n",
       "33. -0.0881761793129557\n",
       "34. -0.0573231282324282\n",
       "35. -0.10876714017781\n",
       "36. -0.118435419114751\n",
       "37. -0.146465937825056\n",
       "38. -0.147771465945962\n",
       "39. -0.0689078994130422\n",
       "40. -0.14472911852288\n",
       "41. 0.114001713771491\n",
       "42. 0.141231178762079\n",
       "43. 0.12466368412858\n",
       "44. 0.0204794069441363\n",
       "45. 0.14388881758651\n",
       "46. 0.10189353412941\n",
       "47. 0.132623260586525\n",
       "48. 0.00748785916154172\n",
       "49. 0.148072279278216\n",
       "50. 0.0433295244440317\n",
       "51. 0.00237309385311468\n",
       "52. 0.133629545859968\n",
       "53. 0.0262974100343439\n",
       "54. 0.140254439235847\n",
       "55. 0.083932675639703\n",
       "56. 0.141463998563181\n",
       "57. 0.126244001337157\n",
       "58. 0.0774407111193878\n",
       "59. 0.0475744696923024\n",
       "60. 0.0371999986218945\n",
       "61. 0.147224630952339\n",
       "62. 0.111889008011961\n",
       "63. 0.107985162769522\n",
       "64. 0.128918256005407\n",
       "65. 0.142970292796558\n",
       "66. 0.147678248488869\n",
       "67. 0.14813014531398\n",
       "68. 0.13857018792774\n",
       "69. 0.133606943632327\n",
       "70. 0.0430671328497398\n",
       "71. 0.02352873985038\n",
       "72. 0.02168028386704\n",
       "73. 0.073878764872419\n",
       "74. 0.124955006986414\n",
       "75. 0.115346391757051\n",
       "76. 0.138077421116358\n",
       "77. 0.13643788165986\n",
       "78. 0.0612021019415432\n",
       "79. 0.113813852551577\n",
       "80. 0.0365514478016036\n",
       "81. 0.0962196158594576\n",
       "82. 0.11707363821231\n",
       "83. 0.0986374987724957\n",
       "84. 0.146586124589798\n",
       "85. 0.128435504823203\n",
       "86. 0.0607021373566533\n",
       "87. 0.0260931858422084\n",
       "88. 0.094113111329475\n",
       "89. 0.145886675578954\n",
       "90. 0.0324949004900476\n",
       "91. 0.123351293240234\n",
       "92. 0.145265028851675\n",
       "93. 0.12232824599906\n",
       "94. 0.0790108785061164\n",
       "95. 0.133920332044817\n",
       "96. 0.121079886003763\n",
       "97. 0.136731379506924\n",
       "98. 0.0127539615389493\n",
       "99. 0.0917336632263122\n",
       "100. 0.0493918805194031\n",
       "101. 0.0874886224214287\n",
       "102. 0.115134892001539\n",
       "103. 0.0758612823627466\n",
       "104. 0.136053316278094\n",
       "105. 0.087211349712913\n",
       "106. 0.0713109532867168\n",
       "107. 0.139541781527235\n",
       "108. 0.147979465142391\n",
       "109. 0.147579018081317\n",
       "110. 0.100595335167089\n",
       "111. 0.104951623365178\n",
       "112. 0.0125424262936534\n",
       "113. 0.147409398901295\n",
       "114. 0.144729454326247\n",
       "115. 0.127843208527055\n",
       "116. 0.0647422962010321\n",
       "117. 0.0945049242631224\n",
       "118. 0.131426711830857\n",
       "119. 0.146528034401455\n",
       "120. 0.108252840407067\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] -0.147582547 -0.094420319 -0.106197854 -0.093925731 -0.148143274\n",
       "  [6] -0.096765243 -0.127359060 -0.142416884 -0.060843940 -0.107860409\n",
       " [11] -0.131669784 -0.137915246 -0.088629673 -0.060137766 -0.073560879\n",
       " [16] -0.024685591 -0.108566361 -0.147729953 -0.094292858 -0.133387560\n",
       " [21] -0.147987474 -0.141582839 -0.137950115 -0.142605941 -0.142141460\n",
       " [26] -0.104063221 -0.144400142 -0.148138650 -0.145723192 -0.113158151\n",
       " [31] -0.106171719 -0.148144044 -0.088176179 -0.057323128 -0.108767140\n",
       " [36] -0.118435419 -0.146465938 -0.147771466 -0.068907899 -0.144729119\n",
       " [41]  0.114001714  0.141231179  0.124663684  0.020479407  0.143888818\n",
       " [46]  0.101893534  0.132623261  0.007487859  0.148072279  0.043329524\n",
       " [51]  0.002373094  0.133629546  0.026297410  0.140254439  0.083932676\n",
       " [56]  0.141463999  0.126244001  0.077440711  0.047574470  0.037199999\n",
       " [61]  0.147224631  0.111889008  0.107985163  0.128918256  0.142970293\n",
       " [66]  0.147678248  0.148130145  0.138570188  0.133606944  0.043067133\n",
       " [71]  0.023528740  0.021680284  0.073878765  0.124955007  0.115346392\n",
       " [76]  0.138077421  0.136437882  0.061202102  0.113813853  0.036551448\n",
       " [81]  0.096219616  0.117073638  0.098637499  0.146586125  0.128435505\n",
       " [86]  0.060702137  0.026093186  0.094113111  0.145886676  0.032494900\n",
       " [91]  0.123351293  0.145265029  0.122328246  0.079010879  0.133920332\n",
       " [96]  0.121079886  0.136731380  0.012753962  0.091733663  0.049391881\n",
       "[101]  0.087488622  0.115134892  0.075861282  0.136053316  0.087211350\n",
       "[106]  0.071310953  0.139541782  0.147979465  0.147579018  0.100595335\n",
       "[111]  0.104951623  0.012542426  0.147409399  0.144729454  0.127843209\n",
       "[116]  0.064742296  0.094504924  0.131426712  0.146528034  0.108252840"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S_e = erro*logistic_deriv(outputP_pre)\n",
    "S_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>9.49386988497689</li><li>-6.72864052563728</li><li>11.7309390406259</li><li>11.1660578674989</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 9.49386988497689\n",
       "\\item -6.72864052563728\n",
       "\\item 11.7309390406259\n",
       "\\item 11.1660578674989\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 9.49386988497689\n",
       "2. -6.72864052563728\n",
       "3. 11.7309390406259\n",
       "4. 11.1660578674989\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  9.493870 -6.728641 11.730939 11.166058"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad = vector()\n",
    "for (i in 1:ncol(input)) {\n",
    "    grad[i] = dot(unlist(input[, i]), S_e)\n",
    "}\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso a atualização dos pesos será feito baseado na operação \n",
    "$$\n",
    "W^* = W - LR\\,\\nabla_w J(W) = \\left(w_1 - LR \\dfrac{\\partial J(W)}{\\partial w_1}, w_2 - LR \\dfrac{\\partial J(W)}{\\partial w_2}, \\dots\\right)\n",
    "$$\n",
    "sendo $LR$ o fator denominado *learning rate*, ou **fator de aprendizado**. A sua função é indicar qual o tamanho do passo que será tomado rumo o gradiente descentende, ou seja, quanto maior o seu valor, maior será o valor de $LR \\dfrac{\\partial J(W)}{\\partial w_j}$, e portanto maior será a variação no valor de $w_j$. Podemos definir $LR=1$ por finalidade de simplicidade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1\n",
    "weight_ItoO = weight_ItoO - LR*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>-8.81028786801151</li><li>7.60154430641615</li><li>-11.0408217524778</li><li>-11.0501217891349</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -8.81028786801151\n",
       "\\item 7.60154430641615\n",
       "\\item -11.0408217524778\n",
       "\\item -11.0501217891349\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -8.81028786801151\n",
       "2. 7.60154430641615\n",
       "3. -11.0408217524778\n",
       "4. -11.0501217891349\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  -8.810288   7.601544 -11.040822 -11.050122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_ItoO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, realizamos uma iteração que envolve os processos de feedforward e backpropagation. Assim, treinar uma rede neural consiste essencialmente em repetir o mesmo processo, porém agora utilizando os novos pesos. Nesta situação, se torna cômodo definir uma função que realiza tanto o processo de feedforward quanto o de backpropagation retornando sempre os novos valores de peso que serão usados na próxima iteração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward = function(input, weight_ItoO){\n",
    "    outputP_pre = vector()\n",
    "    outputP = vector()\n",
    "    for (i in 1:nrow(input)) {\n",
    "        outputP_pre[i] = dot(unlist(input[i, ]), weight_ItoO) ## pré-ativação\n",
    "        outputP[i] = logistic(outputP_pre[i]) ## pós-ativação\n",
    "    }\n",
    "    \n",
    "    out <- list(outputP, outputP_pre)\n",
    "    return (out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "backpropagation = function(input, output, outputP, outputP_pre, weight_ItoO, LR, print_erro = FALSE){\n",
    "    erro = outputP - output\n",
    "    if (print_erro == TRUE){\n",
    "        print('erro quadrático médio:')\n",
    "        print(sum(erro^2)/length(erro))\n",
    "    }\n",
    "\n",
    "    S_e = erro*logistic_deriv(outputP_pre)\n",
    "\n",
    "    grad = vector()\n",
    "    for (i in 1:ncol(input)) {\n",
    "        grad[i] = dot(unlist(input[, i]), S_e)\n",
    "    }\n",
    "\n",
    "    weight_ItoO = weight_ItoO - LR*grad\n",
    "    \n",
    "    return(weight_ItoO)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights_ItoO = function(input, output, weight_ItoO, LR = 1, print_erro) {\n",
    "    out = feedforward(input, weight_ItoO)\n",
    "    outputP = unlist(out[1])\n",
    "    \n",
    "    outputP_pre = unlist(out[2])\n",
    "\n",
    "    weight_ItoO = backpropagation(input, output, outputP, outputP_pre, weight_ItoO, LR, print_erro)\n",
    "    \n",
    "    return(weight_ItoO)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível induzir o processo iterativo para acontecer $N$ vezes. Nos estudos de inteligência artificial, este valor de $N$ é chamado de *epoch*. Definiremos uma variável chamada `epoch` como $10$, onde printaremos o erro quadrático médio $\\left(\\dfrac{\\sum_i(y_i-\\hat{y}_i)^2}{n}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.6835820 0.8729038 0.6901173 0.1159361\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.4513118\n",
      "[1] -5.012740  4.910088 -6.348446 -6.583699\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.007688648\n",
      "[1] -4.909135  5.050807 -6.330267 -6.573476\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.006793036\n",
      "[1] -4.796058  5.201613 -6.309693 -6.560429\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.005760199\n",
      "[1] -4.676317  5.359234 -6.287439 -6.545205\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.004650246\n",
      "[1] -4.557061  5.514621 -6.265212 -6.529486\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.003605081\n",
      "[1] -4.447668  5.655726 -6.245238 -6.515529\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.002768588\n",
      "[1] -4.354240  5.774775 -6.228986 -6.504882\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.002180734\n",
      "[1] -4.277160  5.871478 -6.216597 -6.497782\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001790738\n",
      "[1] -4.213655  5.949662 -6.207461 -6.493698\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.00153151\n",
      "[1] -4.160549  6.013635 -6.200857 -6.491968\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001353964\n",
      "[1] -4.115274  6.066874 -6.196190 -6.492041\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001227731\n",
      "[1] -4.075951  6.111928 -6.193020 -6.493503\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001134677\n",
      "[1] -4.041232  6.150623 -6.191027 -6.496054\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001063827\n",
      "[1] -4.010148  6.184282 -6.189974 -6.499474\n",
      "[1] \"erro quadrático médio:\"\n",
      "[1] 0.001008341\n",
      "[1] \"pesos finais:\"\n",
      "[1] -3.981986  6.213877 -6.189689 -6.503599\n"
     ]
    }
   ],
   "source": [
    "set.seed(40)\n",
    "weight_ItoO <- runif(4)\n",
    "\n",
    "epoch = 15\n",
    "weights_ItoO = matrix(ncol = 4, nrow = epoch)\n",
    "for (i in 1:epoch) {\n",
    "    print(weight_ItoO)\n",
    "    weight_ItoO = new_weights_ItoO(input, output_setosa, weight_ItoO, LR = 0.6, print_erro = TRUE)\n",
    "    weights_ItoO[i, ] = weight_ItoO\n",
    "}\n",
    "print('pesos finais:')\n",
    "print(weight_ItoO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na matriz `weights_ItoO` foram salvos os valores dos pesos para cada iteração. Ao plotar elas, é possível visualizar que existe uma tendência para atingir um valor limite, que é chamado de *plateau*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC/VBMVEUAAAABAQECAgIDAwME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\nRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nHqkAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME\nBAQFBQUGBgYHBwcICAgJCQkKCgoLCwsMDAwNDQ0ODg4PDw8QEBARERESEhITExMUFBQVFRUW\nFhYXFxcYGBgZGRkaGhobGxscHBwdHR0eHh4fHx8gICAhISEiIiIjIyMkJCQlJSUmJiYnJyco\nKCgpKSkqKiorKyssLCwtLS0uLi4vLy8wMDAxMTEyMjIzMzM0NDQ1NTU2NjY3Nzc4ODg5OTk6\nOjo7Ozs8PDw9PT0+Pj4/Pz9AQEBBQUFCQkJDQ0NERERFRUVGRkZHR0dISEhJSUlKSkpLS0tM\nTExNTU1OTk5PT09QUFBRUVFSUlJTU1NUVFRVVVVWVlZXV1dYWFhZWVlaWlpbW1tcXFxdXV1e\nXl5fX19gYGBhYWFiYmJjY2NkZGRlZWVmZmZnZ2doaGhpaWlqampra2tsbGxtbW1ubm5vb29w\ncHBxcXFycnJzc3N0dHR1dXV2dnZ3d3d4eHh5eXl6enp7e3t8fHx9fX1+fn5/f3+AgICBgYGC\ngoKDg4OEhISFhYWGhoaHh4eIiIiJiYmKioqLi4uMjIyNjY2Ojo6Pj4+QkJCRkZGSkpKTk5OU\nlJSVlZWWlpaXl5eYmJiZmZmampqbm5ucnJydnZ2enp6fn5+goKChoaGioqKjo6OkpKSlpaWm\npqanp6eoqKipqamqqqqrq6usrKytra2urq6vr6+wsLCxsbGysrKzs7O0tLS1tbW2tra3t7e4\nuLi5ubm6urq7u7u8vLy9vb2+vr6/v7/AwMDBwcHCwsLDw8PExMTFxcXGxsbHx8fIyMjJycnK\nysrLy8vMzMzNzc3Ozs7Pz8/Q0NDR0dHS0tLT09PU1NTV1dXW1tbX19fY2NjZ2dna2trb29vc\n3Nzd3d3e3t7f39/g4ODh4eHi4uLj4+Pk5OTl5eXm5ubn5+fo6Ojp6enq6urr6+vs7Ozt7e3u\n7u7v7+/w8PDx8fHy8vLz8/P09PT19fX29vb39/f4+Pj5+fn6+vr7+/v8/Pz9/f3+/v7////i\nsF19AAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3dCXwV5bn48SeLSUgIq+xEWSwC\nDYiILLKIFmVTQIlUwAXFIij0WutWBaoGq63US/+99W/dqd7WAlK06rUXUK/VIBDLoiAICnIR\nUJA9LIHkvWcDUeKcxHlm5rwnv+/n0zND8vKetzn5SXLOnBkxAFyToBcAJANCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAF\nhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUOBDSMuLAassr/p3ufch\nLRXAMkur/G3ufUjvymHP7wNQdFjerfLfISTgWwgJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEB\nCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASFBxb4/XH3+DTNLKzt8d9E/Nnu5HN8REjSsbtlk\n7C+vqn3OtkqN3nlNWlqWdF3m8aL8REhQcLDVZSWhzZfdzi+vxOiSTvnzDxz98Mc1k6gkQoKC\nZ07dG9l+mvbPSox+oPlX4U15QS8v1+QvQoKC60fGds59sBKjO8YGLZfk+T2JkKCg4ObYTv+7\nKjE69+Xo9kjK214tyHeEBAW39I/ttPpDJUY3fCG63S3FXi3Id4QEBW+lr4xsXz5lYyVGX3JN\ndPuXnAPeLclnhAQNI5q/bszR52vdU5nBC9JeDG8+aX6bt4vyEyFBw8Gb03PbZ2UXVubZb2N+\nkzbst4+Pzx14qJKzl63/4vsvzR+EBB1b/v6Hf3xV2cGLxnRuc9mfyio3ePPIbJFG9yX2twQh\n4Tvsva9HvTajlgS9DLOhSY+XPlv9x8b9jwS9EieEhIp93qZl4dzHLkt/POiFXNI3cgTfp3Uf\nDXolTggJFbu4577w5sn0D4Ndx5epse/Qe7oFuxBnhIQKrZEPojsX3Ow80GvvpMS+F+bVCXYh\nzggJFfpz49jOAz0CXYd5T2IvNs2uH+xCnBESKvRMi9jObzsHug6zJ+O16M7EC4NdiDNCQoXe\nSd8R3bn6x8EuxIzJ3xnevJc1K+CFOCIkVOho64mR7crMlwJeyVdnnTZ9wbyfZ42v3Iu9ASEk\nVOyNzGuKD332xKlXBr0QU/LLjhm1e/856GU4IyR8h/e6iUjdaQnxMujRoBcQFyHhO+1atL6S\nR/GAkAAFhAQoICRAASEh2Xy1eKv/d0pISC6v5YvI6c/4fbeEhKTyp7RbVpSsmZZ1r8/3S0hI\nJl/VfiSyfSltlb93TEhIJs82ir12222Kv3dMSEgm91wU2xnv87G2hIRkcm+f2M51V/t7x4SE\nZPJy9q7I9mjLf/f3jv0PqXzHlrgHcBESvp/DZ4yOHGV7T50vfb5jf0N6a3SzU0TSmv/4Lcdh\nhITv6V/1O8/4+6M/qvGqz/fra0gHB4g07TZoUPc8kUuczrJJSPi+Pv9pp5x2133k9936GtJU\nGRC7RtuqkVLoMJCQvFFW/Oyzxbwzwgu+htS97fF3iZX3Oc9hICF54v18adFC8t8Peh3JyNeQ\nal379f7dtRwGEpIX1tQetdWYraNqrw16JUnI15B6tPv6LcMXOp0ujZC8cPnFkdOHlF08POiV\nJCFfQ7pXBsdO3/nx1XKfw0BC8kBp1ivRnb9nlQa7kmTk77N2g0RO6zVkaJ9WIo7XxiEkD2yR\n2I90a2RLsCtJRj6/jvTmyCZpImlNRix0HEZIHtgni6I7RSn7g11JMvL/yIaybVs5siEQnW+P\nbm8L+CTESSmIY+3K1q6Kc7I0QvLC7Iy54c3cjDlBryQJ+RrS5KdCN6WF2SIZ1+90GkhInngw\nrc/tt/dJezDodSQjX0OSvqGbcVJ3+I09pI3TleEJyRsr7hw06M4VQa8iKfke0oqUrttDuzNl\nssNAQoI/Prn1wh8WPK7wcoDvIT0Wu8OeXRwGEhJ88bfs7lN/P75e912uZ/I9pKkSuTSpGZ/j\nMJCQ4IdPsyKHTm/Nv8L1VL6H9JysjOwPy3MYSEjww8+7RreLUja6ncrfkJoWzl7SoCB8xFdR\nutMBX4QEP/S8P7Zz6l/dTuVrSHkpEvaaMZMys5Y5DCQk+KHTsRM7tHza7VT+viB7YOWLD43t\nPd+Yxh0WO40jJPhh2Ljodk/GG26nCugsQusr+Nience9TkjwwX/W3BDZTm3i+gnwxDkd1/ro\nz30xJZ7cB3Cisn4tXj1sttyVPtf1VIkTkvnsk+OmxZ4jBzy1/6aM9HrS4mX3MwUV0q5OnRw+\n+xghwR+73p67SuNSz0GFtEOcZiEkWCaokEoXLHD4LCHBMgn0O9IJCAmWCSSkL4rjdEJIsIy/\nIW0cM8OYonyRlAGOBzcREizja0jr6svDZnVmav8JfaXhDoeBhATL+BpSQdo8Y4alhQ/HmCM3\nOQwkJFjG15AaDQ3dNBsc2e/X3mEgIcEyvoaUMzp00/CGyP64XIeBhATL+Hs1iia7jbk0ckhD\nWb7T1SgICZbxNaRZ0rXILM+dWmYOTnQ8+QkhwTL+Pv39QLrk9W4tDbrUkp5Op80lJFjG5xdk\nN09plysi9S6a63igICHBMgEc2bB3k9OFKCIICZbhWDug8g5/11u3gwjplYJ4IwgJCejwr9ql\np7f7VYUtBRHSjLgTEBISz4HzG09/++2HG59f0WnrCQmonKnNNoc3m5v9soJPEhJQKeVNH43u\nPNq0/OTPEhJQKTskdiXxlVLBOxeCCKlkW7wRhISE86V8GN35QLaf/Fme/gYqpbzRk9GdJxol\nyI928RFSFRwMegHVxZ0tvwxvvmxxZwWfJCS7vXpB7ZRW4+P+rAwF+85t8fjy5X9scW5F35yE\nZLVp6TfNK3r87MYfB72QaqHkrmYiTe+q8HTahGSzxakvhTelA3sEvZLqYufO7/gEIdlsbPRt\n+2a9cKnygBGSzbr+OrbT7PlA1wFCslqX6bGd02YGug4QktWuil2N+/PUJcEuBIRks4Vp74Q3\n5aM6VPASIfxESFabmF24ZMOr/Wu9H/RCqj1Cslr5k21TJHvo2qDXAUKy3b6NZUEvAYQEqCAk\nQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAk\nQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAk\nQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAk\nQAEhAQoICVBASIACQgIUEBKggJASzsa/3P/MR0EvAlVESAnmyC3pTXq3SLlyb9ALQZUQUoKZ\n2OC/QrdLfzAw6IWgSggpsXyc+kZkuy7z9YBXgiohpMQy48zYzuBJga4DVURIieWuAbGdmwsC\nXQeqiJASy687x3au+Emg60AVEVJiWZIafeb7q9p/DnglqBJCSjCD8jeGbr+6ML806JWgKggp\nwey6IHPApKG1OmwMeiGoEkJKNOWv3nH5LS/w75Fl/A5p25oj0Z3tmx1GVeeQYCV/Q1qaL9Lo\nqchuP6dZCAmW8TWkT2uk9huUKTPC+4SEZOJrSKNTXg39cNcqY5UhJCQXX0Nq3T98uyZrsCEk\nJBdfQ8qeENncJW8REpKLryG17x7Z7GnSag8hIan4GtK/yZ0l4e08GbaLkJBMfA1pV0vJjPya\n9AvJrU9ISCL+vo60f0qPsyI7z7QRQkISCeoQofINCxw+S0iwTBAhla1ddcR5BCHBMr6GNDl8\ndFBpYbZIxvU7nQYSEizja0jSN3QzTuoOv7GHtDngMJCQYBnfQ1qR0nV7aHemTHYYSEiwjO8h\nPRa7w55dHAYSEizje0hTY42Mz3EYSEiwjO8hPScrI/vD8hwGEhIs429ITQtnL2lQUB7aLUof\n7jCQkGAZX0PKS5Gw14yZlJm1zGEgIcEy/r4ge2Dliw+N7T3fmMYdFjuNIyRYJqBDhNZX8LE9\nO497hJBgl8Q5Hdf66M99MYQEqyROSGbjJ8dNIyTYJaiQdnXq5PBZfkeCZYIKaQfvR0IyCSqk\n0gW8HwlJJIF+RzoBIcEygYT0RXGcTggJlvE3pI1jZhhTlC+SMsDxsiWEBMv4GtK6+vKwWZ2Z\n2n9CX2m4w2EgIcEyvoZUkDbPmGFpb4R258hNDgMJCZbxNaRGQ0M3zQZH9vu1dxhISLCMryHl\njA7dNLwhsj8u12EgIcEyvobUvcluYy6NHNJQln+ew0BCgmV8DWmWdC0yy3OnlpmDEzn5CZKJ\nv09/P5Aueb1bS4MutaTnfodxhATL+PyC7OYp7XJFpN5Fc486DSMkWCaAIxv2bjoUbwghwTIc\nawcoCCKkVwrijSAkWCaIkGbEnYCQYBlCAhQQEqCAkAAFQYRUsi3eCEKCZXj6G1BASIACQgIU\nEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABW5C+vBkTmfGrwJCgmXchCQnm66z\nKkKCZVyFNGDatxASqilXIZ2UDSGhmnIT0m1vffszJ3/k+yEkWEbnWbsn39FYy9cICZbRCUnG\na6zla4QEy7gK6ZVjZGDoRnFVhATLuArpm099K66KkGAZVyHNzJFrHwqTbqEbxVUREizj7nek\nNR2zn4p8hN+RUL25fLLh4AS5cg8hodpz/azdnNqtFhMSqjv3T39v6HbKw4SEak7hdaTS21MI\nCdWcyguyC6fPV1nMcYQEy/DGPkABIflh57vL4l7JHVYjJO+t6itpkjF+T9DrgIcIyXMf1h6y\n+NCul9p0PRj0SuAdQvJc3yHl4c0XjX8T9ErgHULy2ucp70d3Cs8OdiHwEiF57Z8pR6I7L9cK\ndiHwEiF5bcmx/y8vNAh2IfCSVki78vPPuWqd+/VEJVNIJdlzojvXDwx2IfCSVkg7RLa8eqH7\n9UQlU0hmUov/DW9eTX896JXAO1ohlS5YYMxS9+uJSqqQSi6od+ufHhuZdm/QC4GH+B3Je0ce\nuzjvzBFvBr0MeImQAAWcaRVQQEiAAlchZdX5FkJCNeUmpH4nm6WzKkKCZXiyAVBASIACtyEt\n+PlFnXuP+Uup3orCCAmWcRfSsrNi5/0+/U3FNRESrOMqpLcypcuzi3esnjNU0jQvRkFIsI2b\nkHY3SHmqPLr7akYdpQuaRxASLOMmpGky+fjHpst9SisKIyRYxk1I52Z+/RxDWe65SisKIyRY\nxk1Idbqd8MFedVXWE0VIsIybkDIHnfDBIVkq64kiJFjGTUgtGp/wwdNbqKwnipBgGTchXSfv\nHP/YcrlaaUVhhATLuAlpcUresee8D/5Q3lJbEyHBOq5ekP2pNJ8ZeSHp9XZyleKiCAm2cRVS\n2SSROt1HXtBY5ArVqy0QEizj8qDV/7mkhoik9ZyjuCRDSLCO67dRHFr97gr1C5YQEiyj8X6k\n/Wu1SyIkWMZ1SHvvbRL64a7xVNXvfEKCZdyGdKC9NL385oLm8kPNy2gREizjNqTbZcrh0KZ0\nityltiZCgnXchnT2sYO+u52jsp4oQoJl3IaUMy62M6GmynqiCAmWcRtSfu/YTt+OKuuJIiRY\nxm1IN8nvI9v/LzcrrSiMkGAZtyHtOl06Tpo26Sw5fZfeoggJtnH9OtKWcaeISPpPPldbkiEk\nWEfhyIbSj9/6mBNEonpzG9KOY0d9l+xUWU8UIcEybkOSZ2M799dXWU8UIcEyrkJ6/vnnZdzz\nEU+fU0NxVYQEy7gKSU40RHFVhATLuApp3rx58tN5Uf/QfIssIcEybn9H6veP73OvT77j/HlC\ngmWCudCYjHf+PCHBMr6G9MoxMjB04zCQkGAZNyGd8U2V+Ivf4DCQkGAZNyFJpcOImZkj1z4U\nJt1CNw4DCQmW8fd3pDUds5+KzMDvSEguuiHdFu+vHpwgV+4hJCQd3ZAq8W/VnNqtFhMSko3v\nIZkN3U55mJCQZPwPyZTenkJISDIBhGTMwunznQcQEiwTREhla1cdcR5BSLCMryFNDj/3XVqY\nLZJxveP7AAkJlvE1JOkbuhkndYff2EPaHHAYSEiwjO8hrUjpuj20O1MmOwwkJFjG95Aei91h\nzy4OAwkJlvE9pKmxRsbnOAwkJFjG95Cek5WR/WF5DgMJCZbRCal8U/TqSNPj/MWmhbOXNCgI\nXwi9KH24w0BCgmVch/T22I/N9rMl4/ay+H8xLyXyfovXjJmUmbXMYSAhwTJuQ3o9VYrNBDm/\nx/ET3Dk5sPLFh8b2nm9M4w6LncYREizjNqTetd8tP1q/vTmSd15Vplhfwcf27jzuEUKCXdyG\nVO9aY4rlfmNGNXC5kvUpJ77dlpBgFbch1R5pzK/lf0K/9Tg9nV0pq4uPu5uQYBe3IXVtdPBo\n27pHzNGz2lZpjl2dOjl8lt+RYBm3Ic2UH7SWW8w/u8s9VZpjB2cRQjJx/fT3rxqkDdxtHpIB\nu6s0R+mCBQ6fJSRYRuNCY6H/fbpOZzkxhATLBHKhsS+K43RCSLCMvxca2zhmhjFF+SIpAzY6\njSMkWMbXC42tqy8Pm9WZqf0n9JWGOxwGEhIs4+uFxgrS5hkzLO2N0O4cuclhICHBMr5eaKzR\n0NBNs8GR/X7tHQYSEizj64XGckaHbhreENkfl+swkJBgGV9Pot+9yW5jLo0c0lCW73SQKyHB\nMq5DevGqATHx/+Is6VpkludOLTMHJ3LyEyQTtyE9IZJRM6oSf/OBdMnr3VoadKklPfc7jCMk\nWMZtSO1zF1birbHHbZ7SLldE6l0096jTMEKCZdyGlD2xyn9976a4T/AREizjNqSzf6a3lq8R\nEizjNqT7WlbtqO+wVwrijSAkWMZNSPtD9g7tOGfjvvCe05MH3zQj7vPnhATL+HpV8xhCQtJx\nE9IN31Tpv09ISDq+HtkQQ0hIOkGEVLIt3ghCgmXchtT9mIuuvsPx5KlVQkiwjNuQejUVkZqh\n/3VokyUD41wattIICZZxG9LG+l1e228OLDyvz8ED06RQaVWEBMu4DWlkXvQF2X0tbzXmMqer\n8FWFDSFp/euLpOA2pGbXxXZ+0taY6bVV1mRBSP/Zq3ZGh6klQS8DCcNtSHmDYztDGhlz26kq\na0r4kMrH1bjtpYXTT+/4VdArQaJwG9KV6f8d2b55ynBzOL+X0qoSPKS/ZkWeoNyVf23AC0HC\ncBvSpoapw3/3wu9HpNVZv7OtzFJaVYKHdMGk6Pa1jD3BLgQJw/ULsh8Nixxo96NlZnOj32mt\nKsFDqjcnui2R94JdCBKGwpENm+Y/9Wr4Cnxl5TpLMgkfUu150e2hlKp/8ZCcXIW0YMHh/V9T\nXFWCh9Qtdgmbd9K+DHYhSBguz7S67fu8jSK+BA/p0bqfhjelfS4NeiVIFK5C6tRpx/d5G0V8\nCR7SkQGNHlv12bzuTR2vBIDqJIijv+NL8JBM6X1NRHJGfR70OpAwFEI6+MEipcUcl+ghhXz5\nSVVOQ4Zk5zqkz0ZkhH49erTgM7UlGStCAk7kNqStp0mvi8TMTm+yWW9RhATbuA1pojxpng99\n4L3M8XqLIiTYxm1Ip/cxkZBMwQ/U1kRIsI7bkHJujIU0KUdtTYQE67gNqWuXWEg9z1FbEyHB\nOm5DKpT7ysIh/U7u1FsUIcE2bkM60kvO6CFjO8gPD+gtipBgG9evIx1+JE9E6t+9V21JhpBg\nHbchhY/53rdK+y3XhATLuA0ps9/DH+it5hhCgmXchtQ29HNds+tn7dRbURghwTKuf0f6Yvak\njimS1uM+vRMWExKso/I2ip0v/ezU6vTGPuDbFEI6snj60PoiWieHDCMkWMZtSAvv/VGOSN0h\nvy0+qrcoQoJt3IYkUu/y3y3XO39QFCHBMm5DSpdTut/2N+2T6RASLOM2pJI37r+4lsiZY59Z\np7coQoJtNJ61K1v2+yvr86wdqjONkLa/OKlDivB+JFRjbkOKRpRy1h0LD+stipBgG7chpYg0\nGDVzq96CIggJlnEb0vkPFGs/920ICdbhTKuAAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiA\nAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiA\nAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQAAWEBCggJEABIQEKCAlQEEhI\nXxTH6YSQYBl/Q9o4ZoYxRfkiKQM2Oo0jJFjG15DW1ZeHzerM1P4T+krDHQ4DCQmW8TWkgrR5\nxgxLeyO0O0duchhISLCMryE1Ghq6aTY4st+vvcNAQoJlfA0pZ3TopuENkf1xuQ4DCQmW8TWk\n7k12G3Npp/BuWf55DgMJCZbxNaRZ0rXILM+dWmYOTpTJDgMJCZbx9+nvB9Ilr3dradCllvTc\n7zCOkGAZn1+Q3TylXa6I1Lto7lGnYYQEywRwZMPeTYfiDSEkWCaIQ4TK1q464jyCkGAZX0Oa\n/FToprQwWyTj+p1OAwkJlvE1JOkbuhkndYff2EPaHHAYSEiwjO8hrUjpuj20O5Onv5FMfA/p\nsdgd9uziMJCQYBnfQ5oaa2R8jsNAQoJlfA/pOVkZ2R+W5zCQkGAZf0NqWjh7SYOC8tBuUfpw\nh4GEBMv4GlJeioS9ZsykzKxlDgMJCZbx9wXZAytffGhs7/nGNO6w2GlcACFt+cWP2vR/cI/f\nd4skEdBZhNZX8LG9O497xPeQiup3nPLEHS1aVrQwIK7EOR3X+ujPfTF7PbmP77Sn0bjwUbQl\nAzo5HkwLfIfECcmsKj7ubr//RXq0afRA2m0Z/+3vHSNJBBXSrk6dHD7r++9I114T2+lR6O8d\nI0kEFdIOcZrF95CuOHZOo4t/4e8dI0kEFVLpggUOn/U9pDv6RrflzZ/w946RJBLod6QT+B7S\n0tS3I9tnamz1946RJDj3d9TNdZ7ebb78deYMn+8XSYJzf0cd/VUtqSWNn/b5bpEsOPf3MQff\nf+nDUt/vFUmCc38DCjj3N6CAc38DCjj3N6CAc38DCjj3N6CAc38DCjj3N6CAY+0ABUGE9EpB\nvBGEBMsEEdKMuBMQEixDSIACQgIUEBKgIIiQSrbFG0FIsAxPfwMKCAlQQEiAAkICFBASoICQ\nAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQ\nAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQ\nAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQAAWEBCggJEABIQEKCAlQQEiAAkICFBASoICQ\nAAWEBChI5pDK1r28eK/CPEBcSRzSf50hOamZtxxwPxMQT/KG9HL6rRvN/r/lDSxXWBDgLGlD\nKm12d2S7PucF18sB4knakN48ZXd0Z+zlbqcC4krakJ5uGdt55Gy3UwFxJW1ILzSM7dzf0+1U\nQFxJG9KnsiS60+NnbqcC4krakMxlZ20Pbx7KXO96KiCe5A3pq3Ma/NsfH+idNUthPUAcyRuS\nOfQfQ87sOWmt+4mAuJI4JMA/hAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEB\nCggJUBBISEdWbHAeQEiwjL8h7X9w2MhF5qM2Iq0XO40jJFjG15B2tRWRWsWtc0ePyKrxmcNA\nQoJlfA3pVvnlpiWdMrJWG7ModazDQEKCZXwNqW34hD6LZXR4v387h4GEBMv4GlL2uNBNidwT\n3r8p22EgIcEy/v6L1Ct0s0SuCu8P5l8kJBGff0cq3Lasc3qNtcYUp13vMJCQYBl/n7VrIyK5\ni1rUGTM6O2uDw0BCgmX8fR1p3/2XFBSZD1qKtChyGkdIsEwgRzaULl3nPICQYJkgQipbu+qI\n8whCgmV8DWnyU6Gb0sJskYzrdzoNJCRYxteQpG/oZpzUHX5jD2njdG1XQoJlfA9pRUrX8FUi\nZspkh4GEBMv4HtJjsTvs2cVhICHBMr6HNDXWyPgch4GEBMv4HtJzsjKyPyzPYSAhwTL+htS0\ncPaSBgXlod2i9OEOAwkJlvE1pLwUCXvNmEmZWcscBhISLOPvC7IHVr740Nje841p3IG3miOZ\nBHQWoYoukLx353GPEBLskjin41of/bkvZr8n9wF4JHFCMh8WH/e0HPbmPgBvBBXSrk6dHD77\nLiHBLkGFtEOcZiEkWCaokEoXLHD4LCHBMgn0O9IJCAmWCSSkL4rjPLtNSLCMvyFtHDPDmKJ8\nkZQBG53GERIs42tI6+rLw2Z1Zmr/CX2l4Q6HgYQEy/gaUkHaPGOGpb0R2p0jNzkMJCRYxteQ\nGg0N3TQbHNnv195hICHBMr6GlBM+fX7DGyL743IdBhISLONrSN2b7Dbm0sghDWX55zkMJCRY\nxteQZknXIrM8d2qZOTjR8eQnhATL+Pv09wPpkte7tTToUkt6Oh3fTUiwjM8vyG6e0i5XROpd\nNPeo0zBCgmUCOLJh76ZD8YYQEizDsXaAgiBCeqUg3ojvCqms+Nlni8tc3j2gL4iQZsSd4DtC\nej9fWrSQ/Pdd3j+gzqaQ1tQetdWYraNqr3W5AECbTSFdfnH41JKm7GKnc0sCQbAopNKsV6I7\nf88qdbkCQFkQIZVsizeiwkHN/0EAAAfYSURBVJC2SOxHujWyxeUKAGUWPf29TxZFd4pSOOsd\nEoxFIZnOt0e3t3X2fAFA1dgU0uyMueHN3Iw5ni8AqBqbQjIPpvW5/fY+aQ96fv9AFVkVkllx\n56BBd67w/O6BqrIrJCBBERKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAk\nQAEhAQoICVBASIACQgIUJGZISwWwzNIqf5t7H5JZXqyizs3PeeWKNp5N/bgUejZ3nz6eTV0o\nj3s2d5srPJv65jo632vLq/5d7kNIShq94NnU03p6NvUe8e4yAWPGeDb1+7LHs7l7TvNs6hca\neTZ1PIRkCOlkhFRVhGQI6WSEVFWEZAjpZIRUVYRkCOlkhFRVhGQI6WSEVFWEZAjpZIRUVYRk\nCOlkhFRVhGQI6WSEVFWEZAjpZIRUVfaElDfXs6l/c4FnU5ekfuDZ3OPGeTb1B6klns19wW88\nm3punmdTx2NPSBuPeDZ1yVbPpjafeDf1zp3eze3hsrd61+iRjZ5NHY89IQEJjJAABYQEKCAk\nQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUWBLS7ls75Pzg6g2e\nzf+svOLJvLN71mwyYr0nU+/6efvs9rftVp/30TrR7aH7z6t13n2HPJnbg4fz2NRhXj2aTuwI\nqaSVdB9/cUqNYo/mX5PjzZf+V9Jk1JC0+p95MPW+M+W8n5wnbbXfblrSPvYdOVjaXtNGBnox\ntwcP5/FlG+8eTUd2hDRV7gjdvpLawZvpD54lnnzpN6d3Df2L8Te5zoO5C+U+E/7CPKQ66z9+\n3Vai35FvyuCj5kh/ecuDudUfzq+nNt49ms7sCKlHZuQ/vP3kC0+mn5B9jSdf+ilSFN48PMOD\nuS+R8IkmNsllqrNmicS+I0dK+Kwt/5KrPJhb/eH8emrj3aPpzI6Qzuof2QySNV7MPkeeesiT\nL307D09qM1zCV8NaIleqznro0KHYz0hNo2tv2syDudUfzq+n9vDRdGZHSFHbMht6cSahDXWu\nNN586XN7rxjSqPnlH3kwtXk3t3PxgaWdchdpT5wf+Y4sS+sV+VO3U8rV547RfTiPTe3do+nM\nopDWtJInPZi2tFurPd586fdK69yzxg5MyyzSn9uYRemhn2cyqn6x03ii35HbZEjkT4Nkh/rc\nUcoPZ2xq7x7NOKwJac8vamT8uxcT337KYuPNl36TyJTQf8/fSM3Xn9t82DJr1N0jM89Q/2E3\n+h25VYZG/jRItqjPHab+cMam9u7RjMOWkF5qKoNWeTHxwpTwiT89+dIfkgZl4W1/D54jKW1V\nO5zQqtw2R5VnPvajXZ/In7qnlanPbbx4OKNTe/hoxmFJSPdIK73nYb9h+vFLwuv/3FivS2Qz\nQfRf/1oq0RN/j5IVyjPHvtmbtIpsTmvuwdxePJzRqb18NJ3ZEdKzMsyrk7rPHx/WTQaOf0d9\n7n61IgcG9E3Zpz71Ohkd2Y6QDcozx77ZR8i60O1q+bEHc3vxcEan9vLRdGZFSOVn5u7y9h68\n+WFgrkwK/WD0ovT3YO7Ts8P/zL2X1Vp74tg3+0K5xoT/wVP9lyM6tycP54nPY/Cj3XfYIPX7\nRW336B68+dIfPU863nhxSsMNHsz9bmb6pTcPTMt6T3vi2Hdk+QD50T19ZbAHc3vycBJSJSw8\n/pPvZo/uwaMv/b57etRsd6M39X865swaba/Tv/zCse/Ig7/sXqu78kGrsWcEvHg4CQlIAoQE\nKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQE\nKCAkQAEhAQoICVBASIACQgIUEBKggJAABYQEKCAkQAEhAQoICVBASIACQgIUEBKggJAABYRk\nBTmjwg/3Ur3mOFwgJCsQUqIjJCsQUqIjJCsQUqIjJCtEQrqhztF7T6uR/2T4Ax9d3qzZFRsi\nIR2Z1r1mi0lbjfkwo2/oj6X59bYGutbqiZCsEA2p9jUj5r92rswy5p81Uy+45rTGp4dCOtxL\nuozrJadvMuZeecaYB+T5oFdbHRGSFaIhySWh203yY1PeOfUlY/ZfKKGQHpF7Qx99QkaEmsqv\nv3191qUBr7V6IiQrxEJaGN6v288skSvDeyvDIeW1Lgvv98o8aMx7qVf1q7slwHVWX4RkhVhI\nn4f3T+1nnpenIx9u3Nzskx7Ph10kH4Q+8DORPwW4zGqMkKwQC2l/eD8U0nT5r8iHz2luVskx\nRaEPrJOcPQEusxojJCt8K6S/hp9VCGnW3HwlY08YNyRTbvJ/dSAkS3wrpPdlVHjv45TQ70j1\nO0RGzH40dPO8zBiR8m5gq6zOCMkK3wrJnJv6qjEHB4WfbLhb/l/og++lDzdmW/0uR7fUan84\n2LVWT4RkhW+H9E7N1IvHnlEz/ILs3h9K75+OyGy8wZjL0/5lzH/IfcGutXoiJCucGFKL60I3\nH13WvPHl//rDtaHdA3ecnd36xv815gX5eeiPZV0zVwe40uqKkAAFhAQoICRAASEBCggJUEBI\ngAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBI\ngAJCAhQQEqCAkAAFhAQoICRAASEBCggJUEBIgAJCAhT8HzeIYc06BeNQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(weights_ItoO[, 1])\n",
    "plot(weights_ItoO[, 2])\n",
    "plot(weights_ItoO[, 3])\n",
    "plot(weights_ItoO[, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a atualização dos pesos é feita com base no valor do gradiente da função custo, verifica-se que este gradiente possui valores cada vez menores a cada iteração que é feita, o que por sua vez implica em valores de função custo cada vez menores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, o resultado final do modelo é um processo de feedforward realizado sobre os pesos ajustados da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feedforward(input, weight_ItoO)\n",
    "\n",
    "outputP = unlist(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 120 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Predição pelo modelo</th><th scope=col>Valores verdadeiros</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>3.173061e-06</td><td>0</td></tr>\n",
       "\t<tr><td>4.792744e-09</td><td>0</td></tr>\n",
       "\t<tr><td>4.539401e-09</td><td>0</td></tr>\n",
       "\t<tr><td>9.782944e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.010619e-08</td><td>0</td></tr>\n",
       "\t<tr><td>1.968410e-07</td><td>0</td></tr>\n",
       "\t<tr><td>2.484153e-07</td><td>0</td></tr>\n",
       "\t<tr><td>3.409672e-08</td><td>0</td></tr>\n",
       "\t<tr><td>2.678211e-16</td><td>0</td></tr>\n",
       "\t<tr><td>2.286940e-09</td><td>0</td></tr>\n",
       "\t<tr><td>4.373144e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.618189e-06</td><td>0</td></tr>\n",
       "\t<tr><td>1.208774e-13</td><td>0</td></tr>\n",
       "\t<tr><td>7.397525e-08</td><td>0</td></tr>\n",
       "\t<tr><td>2.622571e-07</td><td>0</td></tr>\n",
       "\t<tr><td>2.571508e-08</td><td>0</td></tr>\n",
       "\t<tr><td>7.068917e-07</td><td>0</td></tr>\n",
       "\t<tr><td>1.393819e-05</td><td>0</td></tr>\n",
       "\t<tr><td>1.264148e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.650251e-08</td><td>0</td></tr>\n",
       "\t<tr><td>9.842088e-12</td><td>0</td></tr>\n",
       "\t<tr><td>2.055704e-07</td><td>0</td></tr>\n",
       "\t<tr><td>5.385762e-10</td><td>0</td></tr>\n",
       "\t<tr><td>1.973923e-06</td><td>0</td></tr>\n",
       "\t<tr><td>1.213073e-07</td><td>0</td></tr>\n",
       "\t<tr><td>1.326326e-12</td><td>0</td></tr>\n",
       "\t<tr><td>8.201046e-07</td><td>0</td></tr>\n",
       "\t<tr><td>1.671734e-06</td><td>0</td></tr>\n",
       "\t<tr><td>3.201240e-05</td><td>0</td></tr>\n",
       "\t<tr><td>1.657983e-08</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 120 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Predição pelo modelo & Valores verdadeiros\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t ⋮ & ⋮\\\\\n",
       "\t 3.173061e-06 & 0\\\\\n",
       "\t 4.792744e-09 & 0\\\\\n",
       "\t 4.539401e-09 & 0\\\\\n",
       "\t 9.782944e-09 & 0\\\\\n",
       "\t 1.010619e-08 & 0\\\\\n",
       "\t 1.968410e-07 & 0\\\\\n",
       "\t 2.484153e-07 & 0\\\\\n",
       "\t 3.409672e-08 & 0\\\\\n",
       "\t 2.678211e-16 & 0\\\\\n",
       "\t 2.286940e-09 & 0\\\\\n",
       "\t 4.373144e-09 & 0\\\\\n",
       "\t 1.618189e-06 & 0\\\\\n",
       "\t 1.208774e-13 & 0\\\\\n",
       "\t 7.397525e-08 & 0\\\\\n",
       "\t 2.622571e-07 & 0\\\\\n",
       "\t 2.571508e-08 & 0\\\\\n",
       "\t 7.068917e-07 & 0\\\\\n",
       "\t 1.393819e-05 & 0\\\\\n",
       "\t 1.264148e-09 & 0\\\\\n",
       "\t 1.650251e-08 & 0\\\\\n",
       "\t 9.842088e-12 & 0\\\\\n",
       "\t 2.055704e-07 & 0\\\\\n",
       "\t 5.385762e-10 & 0\\\\\n",
       "\t 1.973923e-06 & 0\\\\\n",
       "\t 1.213073e-07 & 0\\\\\n",
       "\t 1.326326e-12 & 0\\\\\n",
       "\t 8.201046e-07 & 0\\\\\n",
       "\t 1.671734e-06 & 0\\\\\n",
       "\t 3.201240e-05 & 0\\\\\n",
       "\t 1.657983e-08 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 120 × 2\n",
       "\n",
       "| Predição pelo modelo &lt;dbl&gt; | Valores verdadeiros &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| ⋮ | ⋮ |\n",
       "| 3.173061e-06 | 0 |\n",
       "| 4.792744e-09 | 0 |\n",
       "| 4.539401e-09 | 0 |\n",
       "| 9.782944e-09 | 0 |\n",
       "| 1.010619e-08 | 0 |\n",
       "| 1.968410e-07 | 0 |\n",
       "| 2.484153e-07 | 0 |\n",
       "| 3.409672e-08 | 0 |\n",
       "| 2.678211e-16 | 0 |\n",
       "| 2.286940e-09 | 0 |\n",
       "| 4.373144e-09 | 0 |\n",
       "| 1.618189e-06 | 0 |\n",
       "| 1.208774e-13 | 0 |\n",
       "| 7.397525e-08 | 0 |\n",
       "| 2.622571e-07 | 0 |\n",
       "| 2.571508e-08 | 0 |\n",
       "| 7.068917e-07 | 0 |\n",
       "| 1.393819e-05 | 0 |\n",
       "| 1.264148e-09 | 0 |\n",
       "| 1.650251e-08 | 0 |\n",
       "| 9.842088e-12 | 0 |\n",
       "| 2.055704e-07 | 0 |\n",
       "| 5.385762e-10 | 0 |\n",
       "| 1.973923e-06 | 0 |\n",
       "| 1.213073e-07 | 0 |\n",
       "| 1.326326e-12 | 0 |\n",
       "| 8.201046e-07 | 0 |\n",
       "| 1.671734e-06 | 0 |\n",
       "| 3.201240e-05 | 0 |\n",
       "| 1.657983e-08 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "    Predição pelo modelo Valores verdadeiros\n",
       "1   1                    1                  \n",
       "2   1                    1                  \n",
       "3   1                    1                  \n",
       "4   1                    1                  \n",
       "5   1                    1                  \n",
       "6   1                    1                  \n",
       "7   1                    1                  \n",
       "8   1                    1                  \n",
       "9   1                    1                  \n",
       "10  1                    1                  \n",
       "11  1                    1                  \n",
       "12  1                    1                  \n",
       "13  1                    1                  \n",
       "14  1                    1                  \n",
       "15  1                    1                  \n",
       "16  1                    1                  \n",
       "17  1                    1                  \n",
       "18  1                    1                  \n",
       "19  1                    1                  \n",
       "20  1                    1                  \n",
       "21  1                    1                  \n",
       "22  1                    1                  \n",
       "23  1                    1                  \n",
       "24  1                    1                  \n",
       "25  1                    1                  \n",
       "26  1                    1                  \n",
       "27  1                    1                  \n",
       "28  1                    1                  \n",
       "29  1                    1                  \n",
       "30  1                    1                  \n",
       "⋮   ⋮                    ⋮                  \n",
       "91  3.173061e-06         0                  \n",
       "92  4.792744e-09         0                  \n",
       "93  4.539401e-09         0                  \n",
       "94  9.782944e-09         0                  \n",
       "95  1.010619e-08         0                  \n",
       "96  1.968410e-07         0                  \n",
       "97  2.484153e-07         0                  \n",
       "98  3.409672e-08         0                  \n",
       "99  2.678211e-16         0                  \n",
       "100 2.286940e-09         0                  \n",
       "101 4.373144e-09         0                  \n",
       "102 1.618189e-06         0                  \n",
       "103 1.208774e-13         0                  \n",
       "104 7.397525e-08         0                  \n",
       "105 2.622571e-07         0                  \n",
       "106 2.571508e-08         0                  \n",
       "107 7.068917e-07         0                  \n",
       "108 1.393819e-05         0                  \n",
       "109 1.264148e-09         0                  \n",
       "110 1.650251e-08         0                  \n",
       "111 9.842088e-12         0                  \n",
       "112 2.055704e-07         0                  \n",
       "113 5.385762e-10         0                  \n",
       "114 1.973923e-06         0                  \n",
       "115 1.213073e-07         0                  \n",
       "116 1.326326e-12         0                  \n",
       "117 8.201046e-07         0                  \n",
       "118 1.671734e-06         0                  \n",
       "119 3.201240e-05         0                  \n",
       "120 1.657983e-08         0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.000963810562315737"
      ],
      "text/latex": [
       "0.000963810562315737"
      ],
      "text/markdown": [
       "0.000963810562315737"
      ],
      "text/plain": [
       "[1] 0.0009638106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_df = data.frame(matrix(c(outputP, output_setosa), nrow=length(outputP)))\n",
    "names(out_df)[names(out_df) == 'X1'] <- 'Predição pelo modelo'\n",
    "names(out_df)[names(out_df) == 'X2'] <- 'Valores verdadeiros'\n",
    "out_df\n",
    "sum((outputP - output_setosa)^2)/length(outputP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível averiguar que o modelo já realiza predições razoavelmente boas. Porém, a predição obtida na variável `outputP` foi feita sobre o set de **treinamento**, ou seja, sobre os próprios dados que o modelo utilizou para ser otimizado. Com os pesos obtidos, é possível verificar como será a performance sobre novos dados desconhecidos.\n",
    "\n",
    "Para tal, definiremos as variáveis `input_test` e `output_setosa_test`, contendo os dados que foram separados anteriormente e que seriam destinados para realizar testes do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test <- test[c(0:4)]\n",
    "output_setosa_test = distinguir('setosa', test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = feedforward(input_test, weight_ItoO)\n",
    "\n",
    "outputP_test = unlist(out_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 30 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Predição pelo modelo</th><th scope=col>Valores verdadeiros</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>9.999936e-01</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>1.000000e+00</td><td>1</td></tr>\n",
       "\t<tr><td>8.035663e-04</td><td>0</td></tr>\n",
       "\t<tr><td>1.209821e-03</td><td>0</td></tr>\n",
       "\t<tr><td>7.719989e-04</td><td>0</td></tr>\n",
       "\t<tr><td>3.122603e-02</td><td>0</td></tr>\n",
       "\t<tr><td>1.773813e-03</td><td>0</td></tr>\n",
       "\t<tr><td>1.566212e-01</td><td>0</td></tr>\n",
       "\t<tr><td>1.866160e-02</td><td>0</td></tr>\n",
       "\t<tr><td>1.208134e-03</td><td>0</td></tr>\n",
       "\t<tr><td>2.961476e-01</td><td>0</td></tr>\n",
       "\t<tr><td>6.448384e-03</td><td>0</td></tr>\n",
       "\t<tr><td>1.663637e-09</td><td>0</td></tr>\n",
       "\t<tr><td>8.616094e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.730590e-07</td><td>0</td></tr>\n",
       "\t<tr><td>3.508180e-09</td><td>0</td></tr>\n",
       "\t<tr><td>8.640212e-09</td><td>0</td></tr>\n",
       "\t<tr><td>3.815676e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.282227e-09</td><td>0</td></tr>\n",
       "\t<tr><td>1.290948e-07</td><td>0</td></tr>\n",
       "\t<tr><td>6.277904e-06</td><td>0</td></tr>\n",
       "\t<tr><td>1.808569e-05</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Predição pelo modelo & Valores verdadeiros\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 9.999936e-01 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 1.000000e+00 & 1\\\\\n",
       "\t 8.035663e-04 & 0\\\\\n",
       "\t 1.209821e-03 & 0\\\\\n",
       "\t 7.719989e-04 & 0\\\\\n",
       "\t 3.122603e-02 & 0\\\\\n",
       "\t 1.773813e-03 & 0\\\\\n",
       "\t 1.566212e-01 & 0\\\\\n",
       "\t 1.866160e-02 & 0\\\\\n",
       "\t 1.208134e-03 & 0\\\\\n",
       "\t 2.961476e-01 & 0\\\\\n",
       "\t 6.448384e-03 & 0\\\\\n",
       "\t 1.663637e-09 & 0\\\\\n",
       "\t 8.616094e-09 & 0\\\\\n",
       "\t 1.730590e-07 & 0\\\\\n",
       "\t 3.508180e-09 & 0\\\\\n",
       "\t 8.640212e-09 & 0\\\\\n",
       "\t 3.815676e-09 & 0\\\\\n",
       "\t 1.282227e-09 & 0\\\\\n",
       "\t 1.290948e-07 & 0\\\\\n",
       "\t 6.277904e-06 & 0\\\\\n",
       "\t 1.808569e-05 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 2\n",
       "\n",
       "| Predição pelo modelo &lt;dbl&gt; | Valores verdadeiros &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 1.000000e+00 | 1 |\n",
       "| 9.999936e-01 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 1.000000e+00 | 1 |\n",
       "| 8.035663e-04 | 0 |\n",
       "| 1.209821e-03 | 0 |\n",
       "| 7.719989e-04 | 0 |\n",
       "| 3.122603e-02 | 0 |\n",
       "| 1.773813e-03 | 0 |\n",
       "| 1.566212e-01 | 0 |\n",
       "| 1.866160e-02 | 0 |\n",
       "| 1.208134e-03 | 0 |\n",
       "| 2.961476e-01 | 0 |\n",
       "| 6.448384e-03 | 0 |\n",
       "| 1.663637e-09 | 0 |\n",
       "| 8.616094e-09 | 0 |\n",
       "| 1.730590e-07 | 0 |\n",
       "| 3.508180e-09 | 0 |\n",
       "| 8.640212e-09 | 0 |\n",
       "| 3.815676e-09 | 0 |\n",
       "| 1.282227e-09 | 0 |\n",
       "| 1.290948e-07 | 0 |\n",
       "| 6.277904e-06 | 0 |\n",
       "| 1.808569e-05 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   Predição pelo modelo Valores verdadeiros\n",
       "1  1.000000e+00         1                  \n",
       "2  9.999936e-01         1                  \n",
       "3  1.000000e+00         1                  \n",
       "4  1.000000e+00         1                  \n",
       "5  1.000000e+00         1                  \n",
       "6  1.000000e+00         1                  \n",
       "7  1.000000e+00         1                  \n",
       "8  1.000000e+00         1                  \n",
       "9  1.000000e+00         1                  \n",
       "10 1.000000e+00         1                  \n",
       "11 8.035663e-04         0                  \n",
       "12 1.209821e-03         0                  \n",
       "13 7.719989e-04         0                  \n",
       "14 3.122603e-02         0                  \n",
       "15 1.773813e-03         0                  \n",
       "16 1.566212e-01         0                  \n",
       "17 1.866160e-02         0                  \n",
       "18 1.208134e-03         0                  \n",
       "19 2.961476e-01         0                  \n",
       "20 6.448384e-03         0                  \n",
       "21 1.663637e-09         0                  \n",
       "22 8.616094e-09         0                  \n",
       "23 1.730590e-07         0                  \n",
       "24 3.508180e-09         0                  \n",
       "25 8.640212e-09         0                  \n",
       "26 3.815676e-09         0                  \n",
       "27 1.282227e-09         0                  \n",
       "28 1.290948e-07         0                  \n",
       "29 6.277904e-06         0                  \n",
       "30 1.808569e-05         0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.00378685908105597"
      ],
      "text/latex": [
       "0.00378685908105597"
      ],
      "text/markdown": [
       "0.00378685908105597"
      ],
      "text/plain": [
       "[1] 0.003786859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_df = data.frame(matrix(c(outputP_test, output_setosa_test), nrow=length(outputP_test)))\n",
    "names(out_df)[names(out_df) == 'X1'] <- 'Predição pelo modelo'\n",
    "names(out_df)[names(out_df) == 'X2'] <- 'Valores verdadeiros'\n",
    "out_df\n",
    "sum((outputP_test - output_setosa_test)^2)/length(outputP_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível averiguar que, por mais que o resultado ainda seja razoavelmente bom, o modelo não performa da mesma maneira do que para o set de treinamento. Neste exemplo, esta diferença não invalida a rede, pois por mais que o resultado não seja extamente igual ao caso de treinamento, o modelo ainda é capaz de se generalizar para outros casos. Se esta diferença fosse significativa, o modelo teria de ser reestruturado ou otimizado para generalizar em casos externos ao seu conhecimento (set de treinamento). Este problema é conhecido como ***overfit***, ou seja, quando um algoritmo de inteligência artificial não consegue abstrair as informações de dados desconhecidos tão bem quanto é abstraido dos dados de conhecimento. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rede artificial - multicamadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível introduzir novos graus de complexidade no modelo através de uso de mais neurônios, de forma que eles se ligam na intercamada entre os neurônios de entrada e os neurônios de saída. Eles são chamados de ***camadas escondidas*** (ou *hidden layers*) e, diferente dos neurônios vizinhos, eles podem assumir um valor indefinido tanto de neurônios quanto de camadas. A sua presença possibilida introduzir novas operações matriciais que são construidas sobre um novo espaço matemático, possibilitando o reconhecimento de padrões não apenas através de sobreposições lineares.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se um modelo é construido a partir de $P$ camadas, contendo $L$ neurônios em uma camada $p$, então o valor deste neurônio nesta posíção é\n",
    "$$\n",
    "x_{p, l} = f(W_{p-1}^lX_{p-1})\n",
    "$$\n",
    "sendo $W_{p-1} = \\left(w_{p-1, 1}^l, w_{p-1, 2}^l, \\dots w_{p-1, L^*}^l, \\right)$ e $X_{p-1} = \\left(x_{p-1, 1}, x_{p-1, 2}, \\dots , x_{p-1, L^*}\\right)$ os pesos e os valores dos neurônios na camada anterior, respectivamente, possuindo $L^*$ neurônios esta camada. Por exemplo, se fossemos contruir uma rede com duas dimensões de entrada, dois neurônios em uma única camada e um último neurônio de saída, o último neurônio teria o valor:\n",
    "\n",
    "$$\n",
    "\\hat{y} = x_{3, 1} = f(W_2^1X_2) = f(w_{2, 1}^1x_{2, 1} + w_{2, 2}^1x_{2, 2}) = f(w_{2, 1}^1f(W_{1}^1X_{1}) + w_{2, 2}^1f(W_{1}^2X_{1})) = \\\\\n",
    "f(w_{2, 1}^1f(w_{1, 1}^1x_{1, 1}+w_{1, 2}^1x_{1, 2}) + w_{2, 2}^1f(w_{1, 1}^2x_{1, 1}+w_{1, 2}^2x_{1, 2}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por mais complicado que pareça, a introdução de novas camadas em uma rede implica essencialmente em realizar mais operações que já foram discutidas anteriormente: para cada neurônio da camada subsequente, é realizado uma sobreposição linear entre os pesos associados à este neurônio com os neurônios da camada anterior, e então é feito uma operação através da função de ativação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo é apresentado um código que constrói um modelo baseado na presença de apenas uma camada. Esta camada pode aceitar um número inteiro qualquer de neurônios através da variável de entrada `hidden`, realizando o processo de feedforward e backpropagation na quantidade de vezes que é definido em `epoch`. A interface entre a primeira camada (entrada) e a segunda é indicada por `_ItoH` (*input to hidden*), ao passo que entre a segunda camada e a última (saída) é indicado por `_HtoO` (*hidden to output*). O retorno desta função é uma lista, onde\n",
    "\n",
    "- no índice $1$ se encontram os pesos `weight_ItoH`\n",
    "- no índice $2$ se encontram os pesos `weight_HtoO`\n",
    "- no índice $3$ se encontra o erro quadrático médio\n",
    "- no índice $4$ se encontra a quantidade de neurônios da camada intermediária\n",
    "\n",
    "Como o resultado da função é uma lista, os seus objetos também se tornam uma lista e neste sentido se faz necessário o uso da função `unlist` para torná-los em vetores e, caso necessário, posteriormente em matrizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede = function(\n",
    "    input,\n",
    "    output, \n",
    "    hidden, \n",
    "    epoch_count = 1, \n",
    "    lr = 1, \n",
    "    seed = 1, \n",
    "    print_w = FALSE\n",
    ") {\n",
    "    set.seed(seed)\n",
    "    \n",
    "    nInp = ncol(input)\n",
    "    \n",
    "    weight_ItoH <- matrix(runif(nInp*hidden), nrow = hidden, ncol = nInp)\n",
    "    \n",
    "    weight_HtoO <- runif(hidden)\n",
    "    \n",
    "    outputP = vector()\n",
    "    preAtivacao_H = vector()\n",
    "    posAtivacao_H = vector()\n",
    "    \n",
    "    for (epoch in 1:epoch_count) {\n",
    "        if (print_w == TRUE && epoch == 1) {\n",
    "            print(\"--------- EPOCH 0 ---------\")\n",
    "            print(\"Pesos dos inputs: \")\n",
    "            print(weight_ItoH)\n",
    "            print(\"Pesos dos neurônios escondidos: \")\n",
    "            print(weight_HtoO)\n",
    "        }\n",
    "\n",
    "        ## feedforward\n",
    "        for (i in 1:nrow(input)) {\n",
    "            for (j in 1:hidden) {\n",
    "                preAtivacao_H[j] = dot(unlist(input[i,]), weight_ItoH[j,])\n",
    "                posAtivacao_H[j] = logistic(preAtivacao_H[j])\n",
    "            }\n",
    "\n",
    "            preAtivacao_O = dot(posAtivacao_H, weight_HtoO)\n",
    "            posAtivacao_O = logistic(preAtivacao_O)\n",
    "\n",
    "            erro = posAtivacao_O - output[i]\n",
    "\n",
    "            ## backpropagation\n",
    "            for (nodo_H in 1:hidden) {\n",
    "                S_e = erro*logistic_deriv(preAtivacao_O)\n",
    "\n",
    "                grad_HtoO = S_e * posAtivacao_H[nodo_H]\n",
    "\n",
    "                for (nodo_I in 1:nInp) {\n",
    "                    inp = unlist(input[i, nodo_I])\n",
    "                    grad_ItoH = S_e * weight_HtoO[nodo_H] * logistic_deriv(preAtivacao_H[nodo_H]) * inp\n",
    "\n",
    "                    weight_ItoH[nodo_H, nodo_I] = weight_ItoH[nodo_H, nodo_I] - lr * grad_ItoH\n",
    "                }\n",
    "\n",
    "                weight_HtoO[nodo_H] = weight_HtoO[nodo_H] - lr * grad_HtoO\n",
    "            }\n",
    "            \n",
    "        }\n",
    "        \n",
    "        erro_quad = sum(erro^2)/nrow(input)\n",
    "        \n",
    "        if (print_w == TRUE) {\n",
    "            print(sprintf(\"--------- EPOCH %s ---------\", epoch))\n",
    "            print(\"Pesos dos inputs: \")\n",
    "            print(weight_ItoH)\n",
    "            print(\"Pesos dos neurônios escondidos: \")\n",
    "            print(weight_HtoO)\n",
    "            print('Erro quadrático médio:')\n",
    "            print(erro_quad)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    out <- list(weight_ItoH, weight_HtoO, erro_quad, hidden)\n",
    "    return(out)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criaremos, como exemplo, uma rede que possui 7 neurônios na camada interna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"--------- EPOCH 0 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "          [,1]       [,2]      [,3]       [,4]\n",
      "[1,] 0.2655087 0.66079779 0.7698414 0.21214252\n",
      "[2,] 0.3721239 0.62911404 0.4976992 0.65167377\n",
      "[3,] 0.5728534 0.06178627 0.7176185 0.12555510\n",
      "[4,] 0.9082078 0.20597457 0.9919061 0.26722067\n",
      "[5,] 0.2016819 0.17655675 0.3800352 0.38611409\n",
      "[6,] 0.8983897 0.68702285 0.7774452 0.01339033\n",
      "[7,] 0.9446753 0.38410372 0.9347052 0.38238796\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1] 0.8696908 0.3403490 0.4820801 0.5995658 0.4935413 0.1862176 0.8273733\n",
      "[1] \"--------- EPOCH 1 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "            [,1]      [,2]       [,3]         [,4]\n",
      "[1,] -0.11937984 1.2031151 0.20589640 -0.298801321\n",
      "[2,]  0.27312673 0.6959348 0.38134201  0.538006416\n",
      "[3,]  0.46175604 0.1464316 0.58473551 -0.004838991\n",
      "[4,]  0.85388622 0.2341073 0.93281775  0.207432336\n",
      "[5,] -0.01451882 0.4905685 0.06921184  0.100274732\n",
      "[6,]  0.85637615 0.6222272 0.75905250 -0.015721183\n",
      "[7,]  0.85151968 0.5916445 0.77645128  0.243328691\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  0.3355717 -0.6888919 -0.8512009 -0.9196051 -0.4031818 -0.9107387 -0.5211498\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 9.606891e-06\n",
      "[1] \"--------- EPOCH 2 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "            [,1]       [,2]       [,3]        [,4]\n",
      "[1,] -0.42346967  1.5300805 -0.2308352 -0.71011135\n",
      "[2,]  0.39567583  0.4427927  0.6065116  0.74886157\n",
      "[3,]  0.59360673 -0.2272116  0.8727946  0.26215278\n",
      "[4,]  0.90215201 -0.1425177  1.1462767  0.39878348\n",
      "[5,] -0.04211731  0.6304469 -0.0142766  0.02767642\n",
      "[6,]  0.98229266  0.2384501  1.0470366  0.24995530\n",
      "[7,]  0.90608810  0.3969230  0.9147906  0.37021365\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  1.7048966 -0.7410013 -1.1459780 -1.3603480  0.2122458 -1.1129156 -0.7279631\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 1.869345e-06\n",
      "[1] \"--------- EPOCH 3 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "            [,1]        [,2]        [,3]        [,4]\n",
      "[1,] -0.45070330  1.66678552 -0.34721800 -0.82573204\n",
      "[2,]  0.41373594  0.31658699  0.68536756  0.82276778\n",
      "[3,]  0.58791053 -0.40143789  0.95675209  0.34100198\n",
      "[4,]  0.87014103 -0.31892528  1.20729916  0.45462372\n",
      "[5,] -0.07591567  0.72454246 -0.09474282 -0.05012575\n",
      "[6,]  0.98236869  0.07927333  1.12765075  0.32465507\n",
      "[7,]  0.91451915  0.28760106  0.97756779  0.42864372\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.2212327 -0.8250723 -1.3279749 -1.5477553  0.4881063 -1.2363682 -0.8215310\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 6.449971e-07\n",
      "[1] \"--------- EPOCH 4 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "            [,1]          [,2]       [,3]        [,4]\n",
      "[1,] -0.45084436  1.739701e+00 -0.4009797 -0.88084564\n",
      "[2,]  0.41365212  2.516818e-01  0.7198041  0.85551882\n",
      "[3,]  0.56988206 -4.956469e-01  0.9926092  0.37559979\n",
      "[4,]  0.83876923 -4.167037e-01  1.2326968  0.47874950\n",
      "[5,] -0.09551998  7.766117e-01 -0.1424107 -0.09696989\n",
      "[6,]  0.97033782  6.105237e-06  1.1602590  0.35558205\n",
      "[7,]  0.91191992  2.319138e-01  1.0055882  0.45523716\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.4459868 -0.8965368 -1.4465111 -1.6634139  0.6098753 -1.3209406 -0.8906360\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 2.916456e-07\n",
      "[1] \"--------- EPOCH 5 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]        [,2]       [,3]       [,4]\n",
      "[1,] -0.4482894  1.78639501 -0.4357420 -0.9172970\n",
      "[2,]  0.4111917  0.20919957  0.7413073  0.8763170\n",
      "[3,]  0.5545108 -0.55770423  1.0151495  0.3979956\n",
      "[4,]  0.8141538 -0.48185982  1.2483661  0.4943087\n",
      "[5,] -0.1090258  0.81091105 -0.1755342 -0.1298609\n",
      "[6,]  0.9592622 -0.05109792  1.1799258  0.3747420\n",
      "[7,]  0.9083660  0.19579658  1.0231817  0.4722772\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.5857305 -0.9510986 -1.5304922 -1.7448282  0.6876010 -1.3822612 -0.9419500\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 1.616009e-07\n",
      "[1] \"--------- EPOCH 6 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]        [,2]       [,3]       [,4]\n",
      "[1,] -0.4456627  1.81937162 -0.4613879 -0.9446529\n",
      "[2,]  0.4084426  0.17826166  0.7567897  0.8915169\n",
      "[3,]  0.5420909 -0.60264882  1.0316058  0.4147500\n",
      "[4,]  0.7946346 -0.52941069  1.2597247  0.5060230\n",
      "[5,] -0.1193664  0.83586521 -0.2008335 -0.1551791\n",
      "[6,]  0.9498939 -0.08792515  1.1938470  0.3886330\n",
      "[7,]  0.9050250  0.16965808  1.0358920  0.4848051\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.6856753 -0.9946109 -1.5946494 -1.8070677  0.7445622 -1.4299697 -0.9824315\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 1.013998e-07\n",
      "[1] \"--------- EPOCH 7 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]       [,2]       [,3]       [,4]\n",
      "[1,] -0.4433394  1.8441615 -0.4816860 -0.9666020\n",
      "[2,]  0.4058396  0.1542624  0.7688401  0.9035009\n",
      "[3,]  0.5319501 -0.6371878  1.0446039  0.4282469\n",
      "[4,]  0.7787411 -0.5661606  1.2686927  0.5155583\n",
      "[5,] -0.1277597  0.8551698 -0.2212660 -0.1757552\n",
      "[6,]  0.9419710 -0.1162500  1.2045878  0.3995723\n",
      "[7,]  0.9020556  0.1494733  1.0458111  0.4947286\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.7627818 -1.0306733 -1.6462264 -1.8572208  0.7894361 -1.4689176 -1.0158094\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 6.904293e-08\n",
      "[1] \"--------- EPOCH 8 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]       [,2]       [,3]       [,4]\n",
      "[1,] -0.4413306  1.8636219 -0.4984699 -0.9849580\n",
      "[2,]  0.4034691  0.1348434  0.7786891  0.9134058\n",
      "[3,]  0.5235098 -0.6648404  1.0553735  0.4396110\n",
      "[4,]  0.7654786 -0.5957150  1.2761465  0.5236799\n",
      "[5,] -0.1348254  0.8707384 -0.2383826 -0.1930826\n",
      "[6,]  0.9351828 -0.1390006  1.2133275  0.4086305\n",
      "[7,]  0.8994363  0.1331972  1.0539367  0.5029624\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.8251816 -1.0614375 -1.6891976 -1.8991167  0.8264042 -1.5017975 -1.0442070\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 4.978672e-08\n",
      "[1] \"--------- EPOCH 9 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]       [,2]       [,3]       [,4]\n",
      "[1,] -0.4395869  1.8793916 -0.5127690 -1.0007484\n",
      "[2,]  0.4013289  0.1186475  0.7870121  0.9218583\n",
      "[3,]  0.5163528 -0.6876553  1.0645856  0.4494625\n",
      "[4,]  0.7541815 -0.6201880  1.2825538  0.5308010\n",
      "[5,] -0.1409239  0.8836775 -0.2530963 -0.2080448\n",
      "[6,]  0.9292826 -0.1578508  1.2206977  0.4163850\n",
      "[7,]  0.8971153  0.1196602  1.0608174  0.5100122\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.8773745 -1.0882582 -1.7259463 -1.9350375  0.8578034 -1.5302392 -1.0689288\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 3.7455e-08\n",
      "[1] \"--------- EPOCH 10 ---------\"\n",
      "[1] \"Pesos dos inputs: \"\n",
      "           [,1]       [,2]       [,3]       [,4]\n",
      "[1,] -0.4380573  1.8924845 -0.5252187 -1.0146127\n",
      "[2,]  0.3993944  0.1048282  0.7942175  0.9292391\n",
      "[3,]  0.5101837 -0.7069170  1.0726457  0.4581802\n",
      "[4,]  0.7443941 -0.6409126  1.2881919  0.5371708\n",
      "[5,] -0.1462842  0.8946786 -0.2659894 -0.2212077\n",
      "[6,]  0.9240867 -0.1738393  1.2270736  0.4231815\n",
      "[7,]  0.8950428  0.1081363  1.0667850  0.5161862\n",
      "[1] \"Pesos dos neurônios escondidos: \"\n",
      "[1]  2.922099 -1.112035 -1.758003 -1.966445  0.885072 -1.555298 -1.090828\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 2.911004e-08\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 7\n",
    "net_setosa = rede(input, output_setosa, n_hidden, epoch_count = 10, print_w = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Pesos ItoH:\"\n",
      " [1] -0.4380573  0.3993944  0.5101837  0.7443941 -0.1462842  0.9240867\n",
      " [7]  0.8950428  1.8924845  0.1048282 -0.7069170 -0.6409126  0.8946786\n",
      "[13] -0.1738393  0.1081363 -0.5252187  0.7942175  1.0726457  1.2881919\n",
      "[19] -0.2659894  1.2270736  1.0667850 -1.0146127  0.9292391  0.4581802\n",
      "[25]  0.5371708 -0.2212077  0.4231815  0.5161862\n",
      "[1] \"Pesos HtoO:\"\n",
      "[1]  2.922099 -1.112035 -1.758003 -1.966445  0.885072 -1.555298 -1.090828\n",
      "[1] \"Erro quadrático médio:\"\n",
      "[1] 2.911004e-08\n",
      "[1] \"Quantidade de neruônios na camada interna:\"\n",
      "[1] 7\n"
     ]
    }
   ],
   "source": [
    "print('Pesos ItoH:')\n",
    "print(unlist(net_setosa[1]))\n",
    "print('Pesos HtoO:')\n",
    "print(unlist(net_setosa[2]))\n",
    "print('Erro quadrático médio:')\n",
    "print(unlist(net_setosa[3]))\n",
    "print('Quantidade de neruônios na camada interna:')\n",
    "print(unlist(net_setosa[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, a predição consistirá em um processo de feedforward baseado nos últimos pesos que foram retornados pelo algoritmo. Com isso é possível definir a função `predizer`, que toma como parâmetro os valores de entrada e a rede treinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predizer = function(\n",
    "    input,\n",
    "    net\n",
    "){\n",
    "    weight_ItoH = net[1]\n",
    "    weight_HtoO = net[2]\n",
    "    n_hidden = unlist(net[4])\n",
    "\n",
    "    weight_ItoH = matrix(unlist(weight_ItoH), n_hidden, 4)\n",
    "    weight_HtoO = unlist(weight_HtoO)\n",
    "    \n",
    "    nInp = ncol(input)\n",
    "    hidden = length(weight_HtoO)\n",
    "    \n",
    "    output = vector()\n",
    "    preAtivacao_H = vector()\n",
    "    posAtivacao_H = vector()\n",
    "    \n",
    "    for (i in 1:nrow(input)) {\n",
    "        for (j in 1:hidden) {\n",
    "            preAtivacao_H[j] = dot(unlist(input[i,]), weight_ItoH[j,])\n",
    "            posAtivacao_H[j] = logistic(preAtivacao_H[j])\n",
    "        }\n",
    "\n",
    "        preAtivacao_O = dot(posAtivacao_H, weight_HtoO)\n",
    "        posAtivacao_O = logistic(preAtivacao_O)\n",
    "    \n",
    "        output[i] = posAtivacao_O\n",
    "        }\n",
    "\n",
    "    return (output)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out = predizer(input, net = net_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 120 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Predição pelo modelo</th><th scope=col>Valores verdadeiros</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9648397</td><td>1</td></tr>\n",
       "\t<tr><td>0.9458934</td><td>1</td></tr>\n",
       "\t<tr><td>0.9617121</td><td>1</td></tr>\n",
       "\t<tr><td>0.9568477</td><td>1</td></tr>\n",
       "\t<tr><td>0.9673251</td><td>1</td></tr>\n",
       "\t<tr><td>0.9626278</td><td>1</td></tr>\n",
       "\t<tr><td>0.9661626</td><td>1</td></tr>\n",
       "\t<tr><td>0.9629187</td><td>1</td></tr>\n",
       "\t<tr><td>0.9471970</td><td>1</td></tr>\n",
       "\t<tr><td>0.9543980</td><td>1</td></tr>\n",
       "\t<tr><td>0.9642669</td><td>1</td></tr>\n",
       "\t<tr><td>0.9642037</td><td>1</td></tr>\n",
       "\t<tr><td>0.9509041</td><td>1</td></tr>\n",
       "\t<tr><td>0.9605200</td><td>1</td></tr>\n",
       "\t<tr><td>0.9654491</td><td>1</td></tr>\n",
       "\t<tr><td>0.9642055</td><td>1</td></tr>\n",
       "\t<tr><td>0.9659680</td><td>1</td></tr>\n",
       "\t<tr><td>0.9636971</td><td>1</td></tr>\n",
       "\t<tr><td>0.9579481</td><td>1</td></tr>\n",
       "\t<tr><td>0.9673966</td><td>1</td></tr>\n",
       "\t<tr><td>0.9548743</td><td>1</td></tr>\n",
       "\t<tr><td>0.9652268</td><td>1</td></tr>\n",
       "\t<tr><td>0.9713291</td><td>1</td></tr>\n",
       "\t<tr><td>0.9491987</td><td>1</td></tr>\n",
       "\t<tr><td>0.9618035</td><td>1</td></tr>\n",
       "\t<tr><td>0.9394995</td><td>1</td></tr>\n",
       "\t<tr><td>0.9590519</td><td>1</td></tr>\n",
       "\t<tr><td>0.9629968</td><td>1</td></tr>\n",
       "\t<tr><td>0.9614489</td><td>1</td></tr>\n",
       "\t<tr><td>0.9591431</td><td>1</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.0044934132</td><td>0</td></tr>\n",
       "\t<tr><td>0.0016168312</td><td>0</td></tr>\n",
       "\t<tr><td>0.0015702520</td><td>0</td></tr>\n",
       "\t<tr><td>0.0024317209</td><td>0</td></tr>\n",
       "\t<tr><td>0.0021867602</td><td>0</td></tr>\n",
       "\t<tr><td>0.0029721873</td><td>0</td></tr>\n",
       "\t<tr><td>0.0024633290</td><td>0</td></tr>\n",
       "\t<tr><td>0.0056175748</td><td>0</td></tr>\n",
       "\t<tr><td>0.0006837399</td><td>0</td></tr>\n",
       "\t<tr><td>0.0021690322</td><td>0</td></tr>\n",
       "\t<tr><td>0.0017453243</td><td>0</td></tr>\n",
       "\t<tr><td>0.0047040247</td><td>0</td></tr>\n",
       "\t<tr><td>0.0007884621</td><td>0</td></tr>\n",
       "\t<tr><td>0.0023431277</td><td>0</td></tr>\n",
       "\t<tr><td>0.0031574113</td><td>0</td></tr>\n",
       "\t<tr><td>0.0020091524</td><td>0</td></tr>\n",
       "\t<tr><td>0.0032878340</td><td>0</td></tr>\n",
       "\t<tr><td>0.0056776602</td><td>0</td></tr>\n",
       "\t<tr><td>0.0014072198</td><td>0</td></tr>\n",
       "\t<tr><td>0.0016368564</td><td>0</td></tr>\n",
       "\t<tr><td>0.0009320491</td><td>0</td></tr>\n",
       "\t<tr><td>0.0071765236</td><td>0</td></tr>\n",
       "\t<tr><td>0.0013282772</td><td>0</td></tr>\n",
       "\t<tr><td>0.0033520660</td><td>0</td></tr>\n",
       "\t<tr><td>0.0022487714</td><td>0</td></tr>\n",
       "\t<tr><td>0.0009084541</td><td>0</td></tr>\n",
       "\t<tr><td>0.0045928597</td><td>0</td></tr>\n",
       "\t<tr><td>0.0035143533</td><td>0</td></tr>\n",
       "\t<tr><td>0.0070897208</td><td>0</td></tr>\n",
       "\t<tr><td>0.0018689782</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 120 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Predição pelo modelo & Valores verdadeiros\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.9648397 & 1\\\\\n",
       "\t 0.9458934 & 1\\\\\n",
       "\t 0.9617121 & 1\\\\\n",
       "\t 0.9568477 & 1\\\\\n",
       "\t 0.9673251 & 1\\\\\n",
       "\t 0.9626278 & 1\\\\\n",
       "\t 0.9661626 & 1\\\\\n",
       "\t 0.9629187 & 1\\\\\n",
       "\t 0.9471970 & 1\\\\\n",
       "\t 0.9543980 & 1\\\\\n",
       "\t 0.9642669 & 1\\\\\n",
       "\t 0.9642037 & 1\\\\\n",
       "\t 0.9509041 & 1\\\\\n",
       "\t 0.9605200 & 1\\\\\n",
       "\t 0.9654491 & 1\\\\\n",
       "\t 0.9642055 & 1\\\\\n",
       "\t 0.9659680 & 1\\\\\n",
       "\t 0.9636971 & 1\\\\\n",
       "\t 0.9579481 & 1\\\\\n",
       "\t 0.9673966 & 1\\\\\n",
       "\t 0.9548743 & 1\\\\\n",
       "\t 0.9652268 & 1\\\\\n",
       "\t 0.9713291 & 1\\\\\n",
       "\t 0.9491987 & 1\\\\\n",
       "\t 0.9618035 & 1\\\\\n",
       "\t 0.9394995 & 1\\\\\n",
       "\t 0.9590519 & 1\\\\\n",
       "\t 0.9629968 & 1\\\\\n",
       "\t 0.9614489 & 1\\\\\n",
       "\t 0.9591431 & 1\\\\\n",
       "\t ⋮ & ⋮\\\\\n",
       "\t 0.0044934132 & 0\\\\\n",
       "\t 0.0016168312 & 0\\\\\n",
       "\t 0.0015702520 & 0\\\\\n",
       "\t 0.0024317209 & 0\\\\\n",
       "\t 0.0021867602 & 0\\\\\n",
       "\t 0.0029721873 & 0\\\\\n",
       "\t 0.0024633290 & 0\\\\\n",
       "\t 0.0056175748 & 0\\\\\n",
       "\t 0.0006837399 & 0\\\\\n",
       "\t 0.0021690322 & 0\\\\\n",
       "\t 0.0017453243 & 0\\\\\n",
       "\t 0.0047040247 & 0\\\\\n",
       "\t 0.0007884621 & 0\\\\\n",
       "\t 0.0023431277 & 0\\\\\n",
       "\t 0.0031574113 & 0\\\\\n",
       "\t 0.0020091524 & 0\\\\\n",
       "\t 0.0032878340 & 0\\\\\n",
       "\t 0.0056776602 & 0\\\\\n",
       "\t 0.0014072198 & 0\\\\\n",
       "\t 0.0016368564 & 0\\\\\n",
       "\t 0.0009320491 & 0\\\\\n",
       "\t 0.0071765236 & 0\\\\\n",
       "\t 0.0013282772 & 0\\\\\n",
       "\t 0.0033520660 & 0\\\\\n",
       "\t 0.0022487714 & 0\\\\\n",
       "\t 0.0009084541 & 0\\\\\n",
       "\t 0.0045928597 & 0\\\\\n",
       "\t 0.0035143533 & 0\\\\\n",
       "\t 0.0070897208 & 0\\\\\n",
       "\t 0.0018689782 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 120 × 2\n",
       "\n",
       "| Predição pelo modelo &lt;dbl&gt; | Valores verdadeiros &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 0.9648397 | 1 |\n",
       "| 0.9458934 | 1 |\n",
       "| 0.9617121 | 1 |\n",
       "| 0.9568477 | 1 |\n",
       "| 0.9673251 | 1 |\n",
       "| 0.9626278 | 1 |\n",
       "| 0.9661626 | 1 |\n",
       "| 0.9629187 | 1 |\n",
       "| 0.9471970 | 1 |\n",
       "| 0.9543980 | 1 |\n",
       "| 0.9642669 | 1 |\n",
       "| 0.9642037 | 1 |\n",
       "| 0.9509041 | 1 |\n",
       "| 0.9605200 | 1 |\n",
       "| 0.9654491 | 1 |\n",
       "| 0.9642055 | 1 |\n",
       "| 0.9659680 | 1 |\n",
       "| 0.9636971 | 1 |\n",
       "| 0.9579481 | 1 |\n",
       "| 0.9673966 | 1 |\n",
       "| 0.9548743 | 1 |\n",
       "| 0.9652268 | 1 |\n",
       "| 0.9713291 | 1 |\n",
       "| 0.9491987 | 1 |\n",
       "| 0.9618035 | 1 |\n",
       "| 0.9394995 | 1 |\n",
       "| 0.9590519 | 1 |\n",
       "| 0.9629968 | 1 |\n",
       "| 0.9614489 | 1 |\n",
       "| 0.9591431 | 1 |\n",
       "| ⋮ | ⋮ |\n",
       "| 0.0044934132 | 0 |\n",
       "| 0.0016168312 | 0 |\n",
       "| 0.0015702520 | 0 |\n",
       "| 0.0024317209 | 0 |\n",
       "| 0.0021867602 | 0 |\n",
       "| 0.0029721873 | 0 |\n",
       "| 0.0024633290 | 0 |\n",
       "| 0.0056175748 | 0 |\n",
       "| 0.0006837399 | 0 |\n",
       "| 0.0021690322 | 0 |\n",
       "| 0.0017453243 | 0 |\n",
       "| 0.0047040247 | 0 |\n",
       "| 0.0007884621 | 0 |\n",
       "| 0.0023431277 | 0 |\n",
       "| 0.0031574113 | 0 |\n",
       "| 0.0020091524 | 0 |\n",
       "| 0.0032878340 | 0 |\n",
       "| 0.0056776602 | 0 |\n",
       "| 0.0014072198 | 0 |\n",
       "| 0.0016368564 | 0 |\n",
       "| 0.0009320491 | 0 |\n",
       "| 0.0071765236 | 0 |\n",
       "| 0.0013282772 | 0 |\n",
       "| 0.0033520660 | 0 |\n",
       "| 0.0022487714 | 0 |\n",
       "| 0.0009084541 | 0 |\n",
       "| 0.0045928597 | 0 |\n",
       "| 0.0035143533 | 0 |\n",
       "| 0.0070897208 | 0 |\n",
       "| 0.0018689782 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "    Predição pelo modelo Valores verdadeiros\n",
       "1   0.9648397            1                  \n",
       "2   0.9458934            1                  \n",
       "3   0.9617121            1                  \n",
       "4   0.9568477            1                  \n",
       "5   0.9673251            1                  \n",
       "6   0.9626278            1                  \n",
       "7   0.9661626            1                  \n",
       "8   0.9629187            1                  \n",
       "9   0.9471970            1                  \n",
       "10  0.9543980            1                  \n",
       "11  0.9642669            1                  \n",
       "12  0.9642037            1                  \n",
       "13  0.9509041            1                  \n",
       "14  0.9605200            1                  \n",
       "15  0.9654491            1                  \n",
       "16  0.9642055            1                  \n",
       "17  0.9659680            1                  \n",
       "18  0.9636971            1                  \n",
       "19  0.9579481            1                  \n",
       "20  0.9673966            1                  \n",
       "21  0.9548743            1                  \n",
       "22  0.9652268            1                  \n",
       "23  0.9713291            1                  \n",
       "24  0.9491987            1                  \n",
       "25  0.9618035            1                  \n",
       "26  0.9394995            1                  \n",
       "27  0.9590519            1                  \n",
       "28  0.9629968            1                  \n",
       "29  0.9614489            1                  \n",
       "30  0.9591431            1                  \n",
       "⋮   ⋮                    ⋮                  \n",
       "91  0.0044934132         0                  \n",
       "92  0.0016168312         0                  \n",
       "93  0.0015702520         0                  \n",
       "94  0.0024317209         0                  \n",
       "95  0.0021867602         0                  \n",
       "96  0.0029721873         0                  \n",
       "97  0.0024633290         0                  \n",
       "98  0.0056175748         0                  \n",
       "99  0.0006837399         0                  \n",
       "100 0.0021690322         0                  \n",
       "101 0.0017453243         0                  \n",
       "102 0.0047040247         0                  \n",
       "103 0.0007884621         0                  \n",
       "104 0.0023431277         0                  \n",
       "105 0.0031574113         0                  \n",
       "106 0.0020091524         0                  \n",
       "107 0.0032878340         0                  \n",
       "108 0.0056776602         0                  \n",
       "109 0.0014072198         0                  \n",
       "110 0.0016368564         0                  \n",
       "111 0.0009320491         0                  \n",
       "112 0.0071765236         0                  \n",
       "113 0.0013282772         0                  \n",
       "114 0.0033520660         0                  \n",
       "115 0.0022487714         0                  \n",
       "116 0.0009084541         0                  \n",
       "117 0.0045928597         0                  \n",
       "118 0.0035143533         0                  \n",
       "119 0.0070897208         0                  \n",
       "120 0.0018689782         0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0010498939462439"
      ],
      "text/latex": [
       "0.0010498939462439"
      ],
      "text/markdown": [
       "0.0010498939462439"
      ],
      "text/plain": [
       "[1] 0.001049894"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_df = data.frame(matrix(c(pred_out, output_setosa), nrow=length(pred_out)))\n",
    "names(out_df)[names(out_df) == 'X1'] <- 'Predição pelo modelo'\n",
    "names(out_df)[names(out_df) == 'X2'] <- 'Valores verdadeiros'\n",
    "out_df\n",
    "sum((pred_out - output_setosa)^2)/length(pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, este resultado foi baseado nos valores usados para treinar a rede. Podemos realizar uma predição sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out_test = predizer(input_test, net = net_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 30 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Predição pelo modelo</th><th scope=col>Valores verdadeiros</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.965407351</td><td>1</td></tr>\n",
       "\t<tr><td>0.676828205</td><td>1</td></tr>\n",
       "\t<tr><td>0.964432023</td><td>1</td></tr>\n",
       "\t<tr><td>0.958674216</td><td>1</td></tr>\n",
       "\t<tr><td>0.963362648</td><td>1</td></tr>\n",
       "\t<tr><td>0.944813522</td><td>1</td></tr>\n",
       "\t<tr><td>0.967670293</td><td>1</td></tr>\n",
       "\t<tr><td>0.961971309</td><td>1</td></tr>\n",
       "\t<tr><td>0.965347676</td><td>1</td></tr>\n",
       "\t<tr><td>0.960914618</td><td>1</td></tr>\n",
       "\t<tr><td>0.018203839</td><td>0</td></tr>\n",
       "\t<tr><td>0.015273509</td><td>0</td></tr>\n",
       "\t<tr><td>0.017942627</td><td>0</td></tr>\n",
       "\t<tr><td>0.083323662</td><td>0</td></tr>\n",
       "\t<tr><td>0.022388344</td><td>0</td></tr>\n",
       "\t<tr><td>0.077612330</td><td>0</td></tr>\n",
       "\t<tr><td>0.040043554</td><td>0</td></tr>\n",
       "\t<tr><td>0.014945776</td><td>0</td></tr>\n",
       "\t<tr><td>0.147600824</td><td>0</td></tr>\n",
       "\t<tr><td>0.030403443</td><td>0</td></tr>\n",
       "\t<tr><td>0.001540889</td><td>0</td></tr>\n",
       "\t<tr><td>0.001815998</td><td>0</td></tr>\n",
       "\t<tr><td>0.002961184</td><td>0</td></tr>\n",
       "\t<tr><td>0.001704908</td><td>0</td></tr>\n",
       "\t<tr><td>0.002092845</td><td>0</td></tr>\n",
       "\t<tr><td>0.001631841</td><td>0</td></tr>\n",
       "\t<tr><td>0.001625375</td><td>0</td></tr>\n",
       "\t<tr><td>0.002395371</td><td>0</td></tr>\n",
       "\t<tr><td>0.006797657</td><td>0</td></tr>\n",
       "\t<tr><td>0.006091200</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Predição pelo modelo & Valores verdadeiros\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.965407351 & 1\\\\\n",
       "\t 0.676828205 & 1\\\\\n",
       "\t 0.964432023 & 1\\\\\n",
       "\t 0.958674216 & 1\\\\\n",
       "\t 0.963362648 & 1\\\\\n",
       "\t 0.944813522 & 1\\\\\n",
       "\t 0.967670293 & 1\\\\\n",
       "\t 0.961971309 & 1\\\\\n",
       "\t 0.965347676 & 1\\\\\n",
       "\t 0.960914618 & 1\\\\\n",
       "\t 0.018203839 & 0\\\\\n",
       "\t 0.015273509 & 0\\\\\n",
       "\t 0.017942627 & 0\\\\\n",
       "\t 0.083323662 & 0\\\\\n",
       "\t 0.022388344 & 0\\\\\n",
       "\t 0.077612330 & 0\\\\\n",
       "\t 0.040043554 & 0\\\\\n",
       "\t 0.014945776 & 0\\\\\n",
       "\t 0.147600824 & 0\\\\\n",
       "\t 0.030403443 & 0\\\\\n",
       "\t 0.001540889 & 0\\\\\n",
       "\t 0.001815998 & 0\\\\\n",
       "\t 0.002961184 & 0\\\\\n",
       "\t 0.001704908 & 0\\\\\n",
       "\t 0.002092845 & 0\\\\\n",
       "\t 0.001631841 & 0\\\\\n",
       "\t 0.001625375 & 0\\\\\n",
       "\t 0.002395371 & 0\\\\\n",
       "\t 0.006797657 & 0\\\\\n",
       "\t 0.006091200 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 2\n",
       "\n",
       "| Predição pelo modelo &lt;dbl&gt; | Valores verdadeiros &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 0.965407351 | 1 |\n",
       "| 0.676828205 | 1 |\n",
       "| 0.964432023 | 1 |\n",
       "| 0.958674216 | 1 |\n",
       "| 0.963362648 | 1 |\n",
       "| 0.944813522 | 1 |\n",
       "| 0.967670293 | 1 |\n",
       "| 0.961971309 | 1 |\n",
       "| 0.965347676 | 1 |\n",
       "| 0.960914618 | 1 |\n",
       "| 0.018203839 | 0 |\n",
       "| 0.015273509 | 0 |\n",
       "| 0.017942627 | 0 |\n",
       "| 0.083323662 | 0 |\n",
       "| 0.022388344 | 0 |\n",
       "| 0.077612330 | 0 |\n",
       "| 0.040043554 | 0 |\n",
       "| 0.014945776 | 0 |\n",
       "| 0.147600824 | 0 |\n",
       "| 0.030403443 | 0 |\n",
       "| 0.001540889 | 0 |\n",
       "| 0.001815998 | 0 |\n",
       "| 0.002961184 | 0 |\n",
       "| 0.001704908 | 0 |\n",
       "| 0.002092845 | 0 |\n",
       "| 0.001631841 | 0 |\n",
       "| 0.001625375 | 0 |\n",
       "| 0.002395371 | 0 |\n",
       "| 0.006797657 | 0 |\n",
       "| 0.006091200 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   Predição pelo modelo Valores verdadeiros\n",
       "1  0.965407351          1                  \n",
       "2  0.676828205          1                  \n",
       "3  0.964432023          1                  \n",
       "4  0.958674216          1                  \n",
       "5  0.963362648          1                  \n",
       "6  0.944813522          1                  \n",
       "7  0.967670293          1                  \n",
       "8  0.961971309          1                  \n",
       "9  0.965347676          1                  \n",
       "10 0.960914618          1                  \n",
       "11 0.018203839          0                  \n",
       "12 0.015273509          0                  \n",
       "13 0.017942627          0                  \n",
       "14 0.083323662          0                  \n",
       "15 0.022388344          0                  \n",
       "16 0.077612330          0                  \n",
       "17 0.040043554          0                  \n",
       "18 0.014945776          0                  \n",
       "19 0.147600824          0                  \n",
       "20 0.030403443          0                  \n",
       "21 0.001540889          0                  \n",
       "22 0.001815998          0                  \n",
       "23 0.002961184          0                  \n",
       "24 0.001704908          0                  \n",
       "25 0.002092845          0                  \n",
       "26 0.001631841          0                  \n",
       "27 0.001625375          0                  \n",
       "28 0.002395371          0                  \n",
       "29 0.006797657          0                  \n",
       "30 0.006091200          0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.00524083016521609"
      ],
      "text/latex": [
       "0.00524083016521609"
      ],
      "text/markdown": [
       "0.00524083016521609"
      ],
      "text/plain": [
       "[1] 0.00524083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_df = data.frame(matrix(c(pred_out_test, output_setosa_test), nrow=length(pred_out_test)))\n",
    "names(out_df)[names(out_df) == 'X1'] <- 'Predição pelo modelo'\n",
    "names(out_df)[names(out_df) == 'X2'] <- 'Valores verdadeiros'\n",
    "out_df\n",
    "sum((pred_out_test - output_setosa_test)^2)/length(pred_out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, os conceitos principais de uma rede neural foram apresentados. Valem algumas observações:\n",
    "\n",
    "- Como é possível perceber, o processo de aprendizado é essencialmente baseado no algoritmo por trás da atualização dos pesos atribuidos a cada neurônio, e por conta disso este processo é as vezes chamado de **aprendizado por peso**\n",
    "- Por mais que os pesos sejam importantíssimos para descrever uma rede, sabe-los de forma explícita não exprime muito conhecimento acerca de seu modelo. Isto é porque, dado um algoritmo de operação e as condições otimizadas, o que é operado por uma rede não é passível de ser compreendido, sendo chamada as vezes **modelo de caixa preta**. Desta forma, as implementações de modelos de redes neurais são usualmente feitas através de códigos escritos sob o paradigma *orientado ao objeto*, onde uma rede neural representa uma **classe**, que contém os seus **atributos** (quantidade de neurônios, quantidade de camadas, taxa de aprendizado, etc.) e seus **métodos** (algoritmo de feedforward, algoritmo de backpropagation, implementação de minimização de função custo, predição, etc.)\n",
    "- O dataset *Iris* é relativamente simples, de forma que poderia ser tratado por outros algoritmos de inteligência artificial. Conforme o problema se torna mais complexo, se faz necessário maior tempo para cada iteração, mais neurônios entre as camadas, outras opções de algoritmos de otimização e mais testes sobre o modelo variando alguns parâmetros que são fixos, chamados de *hiperparâmetros*. Este processo normalmente é baseado na criação de uma malha (*grid*) na base dos hiperparâmetros e calculando um modelo para cada ponto nesta malha. Com isso, a complexidade compuational escala de forma expressiva, o que pode ser equiparado usando uma GPU para rodar processos em paralelo. \n",
    "- Neste exemplo foi implementado uma rede que faz classificação binária. Para classificação entre mais classes, se clas a implementação de outros conceitos, sendo um deles o de *softmax*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
